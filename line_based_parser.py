import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import re
from urllib.parse import urljoin
import json
from datetime import datetime
import pandas as pd
import os

class LineBasedHochmaParser:
    def __init__(self, db_path="bible_database.db"):
        """
        ì¤„ë°”ê¿ˆ ê¸°ë°˜ í˜¸í¬ë§ˆ ì„±ê²½ì£¼ì„ íŒŒì„œ
        - 19:11 (ë‹¨ì¼ ì ˆ)
        - 19:23,24 (ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ì ˆ)  
        - 19:10-14 (í•˜ì´í”ˆìœ¼ë¡œ êµ¬ë¶„ëœ ë²”ìœ„)
        """
        self.base_url = "https://nocr.net"
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session.headers.update(self.headers)
        self.db_path = db_path
    
    def get_book_code(self, book_name):
        """ì„±ê²½ì±… ì´ë¦„ìœ¼ë¡œë¶€í„° book_code ì¡°íšŒ"""
        book_mapping = {
            'ì°½ì„¸ê¸°': 1, 'ì¶œì• êµ½ê¸°': 2, 'ë ˆìœ„ê¸°': 3, 'ë¯¼ìˆ˜ê¸°': 4, 'ì‹ ëª…ê¸°': 5,
            'ì—¬í˜¸ìˆ˜ì•„': 6, 'ì‚¬ì‚¬ê¸°': 7, 'ë£»ê¸°': 8, 'ì‚¬ë¬´ì—˜ìƒ': 9, 'ì‚¬ë¬´ì—˜í•˜': 10,
            'ì—´ì™•ê¸°ìƒ': 11, 'ì—´ì™•ê¸°í•˜': 12, 'ì—­ëŒ€ìƒ': 13, 'ì—­ëŒ€í•˜': 14, 'ì—ìŠ¤ë¼': 15,
            'ëŠí—¤ë¯¸ì•¼': 16, 'ì—ìŠ¤ë”': 17, 'ìš¥ê¸°': 18, 'ì‹œí¸': 19, 'ì ì–¸': 20,
            'ì „ë„ì„œ': 21, 'ì•„ê°€': 22, 'ì´ì‚¬ì•¼': 23, 'ì˜ˆë ˆë¯¸ì•¼': 24, 'ì˜ˆë ˆë¯¸ì•¼ì• ê°€': 25,
            'ì—ìŠ¤ê²”': 26, 'ë‹¤ë‹ˆì—˜': 27, 'í˜¸ì„¸ì•„': 28, 'ìš”ì—˜': 29, 'ì•„ëª¨ìŠ¤': 30,
            'ì˜¤ë°”ëŒœ': 31, 'ìš”ë‚˜': 32, 'ë¯¸ê°€': 33, 'ë‚˜í›”': 34, 'í•˜ë°•êµ­': 35,
            'ìŠ¤ë°”ëƒ': 36, 'í•™ê°œ': 37, 'ìŠ¤ê°€ë´': 38, 'ë§ë¼ê¸°': 39,
            'ë§ˆíƒœë³µìŒ': 40, 'ë§ˆê°€ë³µìŒ': 41, 'ëˆ„ê°€ë³µìŒ': 42, 'ìš”í•œë³µìŒ': 43, 'ì‚¬ë„í–‰ì „': 44,
            'ë¡œë§ˆì„œ': 45, 'ê³ ë¦°ë„ì „ì„œ': 46, 'ê³ ë¦°ë„í›„ì„œ': 47, 'ê°ˆë¼ë””ì•„ì„œ': 48, 'ì—ë² ì†Œì„œ': 49,
            'ë¹Œë¦½ë³´ì„œ': 50, 'ê³¨ë¡œìƒˆì„œ': 51, 'ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ': 52, 'ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ': 53,
            'ë””ëª¨ë°ì „ì„œ': 54, 'ë””ëª¨ë°í›„ì„œ': 55, 'ë””ë„ì„œ': 56, 'ë¹Œë ˆëª¬ì„œ': 57,
            'íˆë¸Œë¦¬ì„œ': 58, 'ì•¼ê³ ë³´ì„œ': 59, 'ë² ë“œë¡œì „ì„œ': 60, 'ë² ë“œë¡œí›„ì„œ': 61,
            'ìš”í•œì¼ì„œ': 62, 'ìš”í•œì´ì„œ': 63, 'ìš”í•œì‚¼ì„œ': 64, 'ìœ ë‹¤ì„œ': 65, 'ìš”í•œê³„ì‹œë¡': 66
        }
        return book_mapping.get(book_name, 999)
    
    def fetch_page(self, url, timeout=10):
        """ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response
        except requests.RequestException as e:
            print(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def is_verse_separator(self, line):
        """ì¤„ì´ ì ˆ êµ¬ë¶„ìì¸ì§€ íŒë‹¨"""
        line = line.strip()
        if not line:
            return False, []
        
        # ì ˆ êµ¬ë¶„ì íŒ¨í„´ë“¤
        patterns = [
            r'^(\d+):(\d+)$',                      # 19:11
            r'^(\d+):(\d+(?:,\d+)+)$',             # 19:23,24
            r'^(\d+):(\d+)-(\d+)$',                # 19:10-14
        ]
        
        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                return True, self.parse_verse_numbers(match.groups())
        
        return False, []
    
    def parse_verse_numbers(self, groups):
        """ë§¤ì¹˜ëœ ê·¸ë£¹ì—ì„œ ì ˆ ë²ˆí˜¸ë“¤ ì¶”ì¶œ"""
        verses = []
        
        if len(groups) == 2:  # ë‹¨ì¼ ì ˆ ë˜ëŠ” ì½¤ë§ˆ êµ¬ë¶„
            chapter = int(groups[0])
            verse_part = groups[1]
            
            if ',' in verse_part:
                # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ì ˆë“¤ (19:23,24)
                verse_numbers = verse_part.split(',')
                for v in verse_numbers:
                    verses.append((chapter, int(v.strip())))
            else:
                # ë‹¨ì¼ ì ˆ (19:11)
                verses.append((chapter, int(verse_part)))
                
        elif len(groups) == 3:  # ë²”ìœ„ (19:10-14)
            chapter = int(groups[0])
            start_verse = int(groups[1])
            end_verse = int(groups[2])
            
            for v in range(start_verse, end_verse + 1):
                verses.append((chapter, v))
        
        return verses
    
    def extract_line_based_commentary(self, soup, url):
        """ì¤„ë°”ê¿ˆ ê¸°ë°˜ ì£¼ì„ ë°ì´í„° ì¶”ì¶œ"""
        article_data = {
            'url': url,
            'title': '',
            'commentary_name': '',
            'book_name': '',
            'chapter': '',
            'verse_commentaries': [],
            'pattern_info': {}
        }
        
        # 1. ì œëª©ì—ì„œ ì£¼ì„ëª…ê³¼ ì„±ê²½ì±… ì •ë³´ ì¶”ì¶œ
        title_found = False
        for h1 in soup.find_all('h1'):
            h1_text = h1.get_text(strip=True)
            if 'ì£¼ì„' in h1_text and ('ì¥' in h1_text or 'ê¶Œ' in h1_text or 'ì„œ' in h1_text):
                article_data['title'] = h1_text
                title_found = True
                break
        
        if not title_found:
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text(strip=True)
                if ' - ' in title_text:
                    article_data['title'] = title_text.split(' - ')[0]
                else:
                    article_data['title'] = title_text
        
        # 2. ì œëª©ì—ì„œ ì£¼ì„ëª…ê³¼ ì„±ê²½ì±… ì •ë³´ íŒŒì‹±
        if article_data['title']:
            title_match = re.search(r'([^,]+),\s*([ê°€-í£]+)\s*(\d+)ì¥', article_data['title'])
            if title_match:
                article_data['commentary_name'] = title_match.group(1).strip()
                article_data['book_name'] = title_match.group(2).strip()
                article_data['chapter'] = title_match.group(3).strip()
        
        # 3. ë³¸ë¬¸ì—ì„œ ì¤„ë°”ê¿ˆ ê¸°ë°˜ ì ˆë³„ ì£¼ì„ ì¶”ì¶œ
        content_element = soup.find(class_='xe_content') or soup.find(class_='rd_body') or soup.find(class_='rhymix_content')
        
        if content_element:
            content_text = content_element.get_text(strip=True)
            lines = content_text.split('\n')
            
            verse_separators = []
            for i, line in enumerate(lines):
                is_separator, verses = self.is_verse_separator(line)
                if is_separator:
                    verse_separators.append((i, line.strip(), verses))
            
            print(f"ğŸ“‹ ë°œê²¬ëœ ì ˆ êµ¬ë¶„ì: {len(verse_separators)}ê°œ")
            
            if verse_separators:
                article_data['pattern_info'] = {
                    'type': 'line_based_verses',
                    'count': len(verse_separators)
                }
                
                # ê° ì ˆ êµ¬ë¶„ìì— ëŒ€í•´ ë‚´ìš© ì¶”ì¶œ
                for j, (line_idx, separator_text, verses) in enumerate(verse_separators):
                    # ë‹¤ìŒ ì ˆ êµ¬ë¶„ìê¹Œì§€ì˜ ë‚´ìš© ì¶”ì¶œ
                    if j + 1 < len(verse_separators):
                        next_line_idx = verse_separators[j + 1][0]
                    else:
                        next_line_idx = len(lines)
                    
                    # ì ˆ êµ¬ë¶„ì ë‹¤ìŒ ì¤„ë¶€í„° ë‚´ìš© ì¶”ì¶œ
                    content_lines = lines[line_idx + 1:next_line_idx]
                    verse_content = '\n'.join(content_lines).strip()
                    
                    # ë¹ˆ ì¤„ ì •ë¦¬
                    verse_content = re.sub(r'\n\s*\n', '\n\n', verse_content)
                    
                    if verse_content and len(verse_content) > 10:
                        # ëª¨ë“  ê´€ë ¨ ì ˆì— ê°™ì€ ë‚´ìš© ì¶”ê°€
                        for chapter, verse in verses:
                            article_data['verse_commentaries'].append({
                                'chapter': chapter,
                                'verse': verse,
                                'commentary': verse_content,
                                'separator': separator_text
                            })
            
            # ì ˆ êµ¬ë¶„ìê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì£¼ì„ìœ¼ë¡œ ì²˜ë¦¬
            elif len(content_text) > 100:
                if article_data['chapter']:
                    article_data['verse_commentaries'].append({
                        'chapter': int(article_data['chapter']),
                        'verse': 1,
                        'commentary': content_text,
                        'separator': 'whole_chapter'
                    })
                    article_data['pattern_info'] = {
                        'type': 'whole_chapter',
                        'count': 1
                    }
        
        return article_data
    
    def parse_to_excel(self, article_id, excel_filename=None):
        """ë‹¨ì¼ ê²Œì‹œê¸€ì„ íŒŒì‹±í•˜ì—¬ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        url = f"{self.base_url}/com_kor_hochma/{article_id}"
        
        print(f"íŒŒì‹± ì¤‘: {url}")
        
        response = self.fetch_page(url)
        if not response:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        article_data = self.extract_line_based_commentary(soup, url)
        
        if not article_data['verse_commentaries']:
            print(f"âœ— ì£¼ì„ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {url}")
            return None
        
        # Excel íŒŒì¼ëª… ìƒì„±
        if not excel_filename:
            pattern_info = article_data['pattern_info'].get('type', 'unknown')
            excel_filename = f"hochma_{article_data['book_name']}_{article_data['chapter']}ì¥_{pattern_info}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        # DataFrame ìƒì„±
        excel_data = []
        for verse_data in article_data['verse_commentaries']:
            excel_data.append({
                'ID': f"{article_data['book_name']}_{verse_data['chapter']}_{verse_data['verse']}",
                'ì£¼ì„ëª…': article_data['commentary_name'],
                'ì„±ê²½ì±…': article_data['book_name'],
                'ì„±ê²½ì±…_ì½”ë“œ': self.get_book_code(article_data['book_name']),
                'ì¥': verse_data['chapter'],
                'ì ˆ': verse_data['verse'],
                'ì£¼ì„_ë‚´ìš©': verse_data['commentary'],
                'ë²„ì „': f"{article_data['commentary_name']}-commentary",
                'ì›ë³¸_URL': article_data['url'],
                'íŒŒì‹±_ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'ë‚´ìš©_ê¸¸ì´': len(verse_data['commentary']),
                'íŒ¨í„´_ìœ í˜•': article_data['pattern_info'].get('type', 'unknown'),
                'ì ˆ_êµ¬ë¶„ì': verse_data.get('separator', '')
            })
        
        df = pd.DataFrame(excel_data)
        
        # Excel íŒŒì¼ë¡œ ì €ì¥
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='ì£¼ì„ë°ì´í„°', index=False)
            
            # ì ˆ êµ¬ë¶„ìë³„ í†µê³„
            separator_stats = df['ì ˆ_êµ¬ë¶„ì'].value_counts().to_dict()
            separator_data = {
                'ì ˆ_êµ¬ë¶„ì': list(separator_stats.keys()),
                'ê°œìˆ˜': list(separator_stats.values())
            }
            separator_df = pd.DataFrame(separator_data)
            separator_df.to_excel(writer, sheet_name='ì ˆêµ¬ë¶„ì_í†µê³„', index=False)
            
            # ìš”ì•½ ì‹œíŠ¸ ìƒì„±
            summary_data = {
                'í•­ëª©': ['ì£¼ì„ëª…', 'ì„±ê²½ì±…', 'ì¥', 'ì´ ì ˆ ìˆ˜', 'ê³ ìœ  ì ˆ ìˆ˜', 'í‰ê·  ë‚´ìš© ê¸¸ì´', 'íŒ¨í„´ ìœ í˜•', 'íŒŒì‹± ë‚ ì§œ', 'ì›ë³¸ URL'],
                'ê°’': [
                    article_data['commentary_name'],
                    article_data['book_name'],
                    article_data['chapter'],
                    len(article_data['verse_commentaries']),
                    df[['ì¥', 'ì ˆ']].drop_duplicates().shape[0],
                    round(df['ë‚´ìš©_ê¸¸ì´'].mean()),
                    article_data['pattern_info'].get('type', 'unknown'),
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    article_data['url']
                ]
            }
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='ìš”ì•½ì •ë³´', index=False)
        
        print(f"âœ“ Excel ì €ì¥ ì™„ë£Œ: {excel_filename}")
        print(f"  - {article_data['book_name']} {article_data['chapter']}ì¥")
        print(f"  - {len(article_data['verse_commentaries'])}ê°œ ì ˆ ({article_data['pattern_info'].get('type', 'unknown')} íŒ¨í„´)")
        print(f"  - ê³ ìœ  ì ˆ ìˆ˜: {df[['ì¥', 'ì ˆ']].drop_duplicates().shape[0]}ê°œ")
        print(f"  - í‰ê·  ë‚´ìš© ê¸¸ì´: {round(df['ë‚´ìš©_ê¸¸ì´'].mean())}ì")
        
        return {
            'excel_file': excel_filename,
            'article_data': article_data,
            'dataframe': df
        }
    
    def excel_to_database(self, excel_file):
        """ì—‘ì…€ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not os.path.exists(excel_file):
            print(f"ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_file}")
            return False
        
        # ì—‘ì…€ íŒŒì¼ ì½ê¸°
        df = pd.read_excel(excel_file, sheet_name='ì£¼ì„ë°ì´í„°')
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° í…Œì´ë¸” ìƒì„±
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # commentaries í…Œì´ë¸” ìƒì„±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS commentaries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                book_name TEXT NOT NULL,
                book_code INTEGER NOT NULL,
                chapter INTEGER NOT NULL,
                verse INTEGER NOT NULL,
                text TEXT NOT NULL,
                version TEXT NOT NULL,
                verse_title TEXT,
                commentary_name TEXT NOT NULL,
                original_url TEXT,
                pattern_type TEXT,
                verse_separator TEXT,
                parsed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ë°ì´í„° ì‚½ì…
        saved_count = 0
        for _, row in df.iterrows():
            cursor.execute('''
                INSERT OR REPLACE INTO commentaries 
                (book_name, book_code, chapter, verse, text, version, verse_title, 
                 commentary_name, original_url, pattern_type, verse_separator, parsed_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                row['ì„±ê²½ì±…'],
                row['ì„±ê²½ì±…_ì½”ë“œ'],
                row['ì¥'],
                row['ì ˆ'],
                row['ì£¼ì„_ë‚´ìš©'],
                row['ë²„ì „'],
                None,
                row['ì£¼ì„ëª…'],
                row['ì›ë³¸_URL'],
                row.get('íŒ¨í„´_ìœ í˜•', 'unknown'),
                row.get('ì ˆ_êµ¬ë¶„ì', ''),
                row['íŒŒì‹±_ë‚ ì§œ']
            ))
            saved_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"âœ“ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì ˆ")
        return True


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = LineBasedHochmaParser()
    
    print("ì¤„ë°”ê¿ˆ ê¸°ë°˜ í˜¸í¬ë§ˆ ì„±ê²½ì£¼ì„ íŒŒì„œ")
    print("ì§€ì› íŒ¨í„´: 19:11, 19:23,24, 19:10-14")
    print("=" * 50)
    
    while True:
        print("\nì„ íƒí•  ì‘ì—…:")
        print("1. ë‹¨ì¼ ê²Œì‹œê¸€ íŒŒì‹±")
        print("2. ì—‘ì…€ íŒŒì¼ â†’ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        print("3. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == '1':
            article_id = input("ê²Œì‹œê¸€ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            excel_filename = input("Excel íŒŒì¼ëª… (ì—”í„°ì‹œ ìë™ìƒì„±): ").strip() or None
            
            if article_id:
                result = parser.parse_to_excel(article_id, excel_filename)
                if result:
                    print(f"\nğŸ“ íŒŒì¼ ìœ„ì¹˜: {result['excel_file']}")
                    
                    save_to_db = input("\në°ì´í„°ë² ì´ìŠ¤ì—ë„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    if save_to_db == 'y':
                        parser.excel_to_database(result['excel_file'])
        
        elif choice == '2':
            excel_file = input("ì—‘ì…€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            parser.excel_to_database(excel_file)
        
        elif choice == '3':
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main() 