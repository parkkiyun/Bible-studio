import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from datetime import datetime
import json

class CorrectedHochmaParser:
    def __init__(self):
        self.base_url = "https://nocr.net/index.php?mid=com_kor_hochma&document_srl="
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # ì„±ê²½ì±… ì •ê·œí™” ë§¤í•‘
        self.book_name_mapping = {
            'ì°½ì„¸ê¸°': 'ì°½ì„¸ê¸°', 'ì¶œì• êµ½ê¸°': 'ì¶œì• êµ½ê¸°', 'ë ˆìœ„ê¸°': 'ë ˆìœ„ê¸°', 'ë¯¼ìˆ˜ê¸°': 'ë¯¼ìˆ˜ê¸°', 'ì‹ ëª…ê¸°': 'ì‹ ëª…ê¸°',
            'ì—¬í˜¸ìˆ˜ì•„': 'ì—¬í˜¸ìˆ˜ì•„', 'ì‚¬ì‚¬ê¸°': 'ì‚¬ì‚¬ê¸°', 'ë£»ê¸°': 'ë£»ê¸°', 
            'ì‚¬ë¬´ì—˜ìƒ': 'ì‚¬ë¬´ì—˜ìƒ', 'ì‚¬ë¬´ì—˜í•˜': 'ì‚¬ë¬´ì—˜í•˜', 'ì—´ì™•ê¸°ìƒ': 'ì—´ì™•ê¸°ìƒ', 'ì—´ì™•ê¸°í•˜': 'ì—´ì™•ê¸°í•˜',
            'ì—­ëŒ€ìƒ': 'ì—­ëŒ€ìƒ', 'ì—­ëŒ€í•˜': 'ì—­ëŒ€í•˜', 'ì—ìŠ¤ë¼': 'ì—ìŠ¤ë¼', 'ëŠí—¤ë¯¸ì•¼': 'ëŠí—¤ë¯¸ì•¼', 'ì—ìŠ¤ë”': 'ì—ìŠ¤ë”',
            'ìš¥ê¸°': 'ìš¥ê¸°', 'ì‹œí¸': 'ì‹œí¸', 'ì ì–¸': 'ì ì–¸', 'ì „ë„ì„œ': 'ì „ë„ì„œ', 'ì•„ê°€': 'ì•„ê°€',
            'ì´ì‚¬ì•¼': 'ì´ì‚¬ì•¼', 'ì˜ˆë ˆë¯¸ì•¼': 'ì˜ˆë ˆë¯¸ì•¼', 'ì˜ˆë ˆë¯¸ì•¼ì• ê°€': 'ì˜ˆë ˆë¯¸ì•¼ì• ê°€', 'ì—ìŠ¤ê²”': 'ì—ìŠ¤ê²”', 'ë‹¤ë‹ˆì—˜': 'ë‹¤ë‹ˆì—˜',
            'í˜¸ì„¸ì•„': 'í˜¸ì„¸ì•„', 'ìš”ì—˜': 'ìš”ì—˜', 'ì•„ëª¨ìŠ¤': 'ì•„ëª¨ìŠ¤', 'ì˜¤ë°”ëŒœ': 'ì˜¤ë°”ëŒœ', 'ìš”ë‚˜': 'ìš”ë‚˜', 
            'ë¯¸ê°€': 'ë¯¸ê°€', 'ë‚˜í›”': 'ë‚˜í›”', 'í•˜ë°•êµ­': 'í•˜ë°•êµ­', 'ìŠ¤ë°”ëƒ': 'ìŠ¤ë°”ëƒ', 'í•™ê°œ': 'í•™ê°œ', 'ìŠ¤ê°€ë´': 'ìŠ¤ê°€ë´', 'ë§ë¼ê¸°': 'ë§ë¼ê¸°',
            'ë§ˆíƒœë³µìŒ': 'ë§ˆíƒœë³µìŒ', 'ë§ˆê°€ë³µìŒ': 'ë§ˆê°€ë³µìŒ', 'ëˆ„ê°€ë³µìŒ': 'ëˆ„ê°€ë³µìŒ', 'ìš”í•œë³µìŒ': 'ìš”í•œë³µìŒ', 'ì‚¬ë„í–‰ì „': 'ì‚¬ë„í–‰ì „',
            'ë¡œë§ˆì„œ': 'ë¡œë§ˆì„œ', 'ê³ ë¦°ë„ì „ì„œ': 'ê³ ë¦°ë„ì „ì„œ', 'ê³ ë¦°ë„í›„ì„œ': 'ê³ ë¦°ë„í›„ì„œ', 'ê°ˆë¼ë””ì•„ì„œ': 'ê°ˆë¼ë””ì•„ì„œ',
            'ì—ë² ì†Œì„œ': 'ì—ë² ì†Œì„œ', 'ë¹Œë¦½ë³´ì„œ': 'ë¹Œë¦½ë³´ì„œ', 'ê³¨ë¡œìƒˆì„œ': 'ê³¨ë¡œìƒˆì„œ', 
            'ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ': 'ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ', 'ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ': 'ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ',
            'ë””ëª¨ë°ì „ì„œ': 'ë””ëª¨ë°ì „ì„œ', 'ë””ëª¨ë°í›„ì„œ': 'ë””ëª¨ë°í›„ì„œ', 'ë””ë„ì„œ': 'ë””ë„ì„œ', 'ë¹Œë ˆëª¬ì„œ': 'ë¹Œë ˆëª¬ì„œ',
            'íˆë¸Œë¦¬ì„œ': 'íˆë¸Œë¦¬ì„œ', 'ì•¼ê³ ë³´ì„œ': 'ì•¼ê³ ë³´ì„œ', 'ë² ë“œë¡œì „ì„œ': 'ë² ë“œë¡œì „ì„œ', 'ë² ë“œë¡œí›„ì„œ': 'ë² ë“œë¡œí›„ì„œ',
            'ìš”í•œì¼ì„œ': 'ìš”í•œì¼ì„œ', 'ìš”í•œì´ì„œ': 'ìš”í•œì´ì„œ', 'ìš”í•œì‚¼ì„œ': 'ìš”í•œì‚¼ì„œ', 'ìœ ë‹¤ì„œ': 'ìœ ë‹¤ì„œ', 'ìš”í•œê³„ì‹œë¡': 'ìš”í•œê³„ì‹œë¡'
        }

    def extract_title_and_info(self, article_id):
        """ê²Œì‹œê¸€ì—ì„œ ì˜¬ë°”ë¥¸ ì œëª©ê³¼ ì„±ê²½ ì •ë³´ ì¶”ì¶œ"""
        url = f"{self.base_url}{article_id}"
        
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return None, None, None, None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # title íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
            title_tag = soup.find('title')
            if not title_tag:
                return None, None, None, None
                
            title_text = title_tag.get_text(strip=True)
            
            # "í˜¸í¬ë§ˆ ì£¼ì„, ì°½ì„¸ê¸° 01ì¥ - í˜¸í¬ë§ˆ ì£¼ì„ - HANGL NOCR" í˜•ì‹ì—ì„œ ì •ë³´ ì¶”ì¶œ
            # íŒ¨í„´: í˜¸í¬ë§ˆ ì£¼ì„, ì„±ê²½ì±…ëª… ìˆ«ìì¥
            pattern = r'í˜¸í¬ë§ˆ ì£¼ì„[,\s]*([ê°€-í£]+(?:ìƒ|í•˜)?(?:ì „ì„œ|í›„ì„œ)?(?:ì¼ì„œ|ì´ì„œ|ì‚¼ì„œ)?(?:ë³µìŒ)?(?:ê¸°)?(?:ì• ê°€)?)\s*(\d+)ì¥'
            match = re.search(pattern, title_text)
            
            if not match:
                return title_text, None, None, None
            
            book_name_raw = match.group(1)
            chapter = int(match.group(2))
            
            # ì„±ê²½ì±…ëª… ì •ê·œí™”
            book_name = self.book_name_mapping.get(book_name_raw, book_name_raw)
            
            return title_text, "í˜¸í¬ë§ˆ ì£¼ì„", book_name, chapter
            
        except Exception as e:
            print(f"âŒ ê²Œì‹œê¸€ {article_id} ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None, None, None

    def parse_article_content(self, article_id):
        """ê²Œì‹œê¸€ ë‚´ìš© íŒŒì‹±"""
        url = f"{self.base_url}{article_id}"
        
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì •ë³´ ì¶”ì¶œ
            title, commentary_name, book_name, chapter = self.extract_title_and_info(article_id)
            
            if not book_name or not chapter:
                print(f"âš ï¸  ê²Œì‹œê¸€ {article_id}: ì„±ê²½ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                return []
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content_selectors = ['.xe_content', '.rd_body', '.rhymix_content']
            content_area = None
            
            for selector in content_selectors:
                content_area = soup.select_one(selector)
                if content_area:
                    break
            
            if not content_area:
                print(f"âš ï¸  ê²Œì‹œê¸€ {article_id}: ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            # <br> íƒœê·¸ë¥¼ \nìœ¼ë¡œ ë³€í™˜
            for br in content_area.find_all('br'):
                br.replace_with('\n')
            
            full_text = content_area.get_text()
            lines = [line.strip() for line in full_text.split('\n') if line.strip()]
            
            # ì ˆë³„ íŒŒì‹±
            verses_data = []
            
            # íŒ¨í„´ 1: ====31:1 ë˜ëŠ” ===31:1 í˜•ì‹
            equals_pattern = r'^={3,}(\d+):(\d+(?:,\d+)*(?:-\d+)*)$'
            
            # íŒ¨í„´ 2: 31:1 í˜•ì‹ (ë…ë¦½ëœ ì¤„)  
            line_pattern = r'^(\d+):(\d+(?:,\d+)*(?:-\d+)*)$'
            
            current_verses = []
            current_content = []
            pattern_type = "none"
            
            for line in lines:
                # íŒ¨í„´ 1: equals êµ¬ë¶„ì
                equals_match = re.match(equals_pattern, line)
                if equals_match:
                    # ì´ì „ ì ˆ ì €ì¥
                    if current_verses and current_content:
                        content_text = '\n'.join(current_content).strip()
                        if content_text:
                            self._add_verses_data(verses_data, article_id, url, title, commentary_name, 
                                                book_name, chapter, current_verses, content_text, "equals")
                    
                    # ìƒˆ ì ˆ ì‹œì‘
                    ch = int(equals_match.group(1))
                    verses_str = equals_match.group(2)
                    current_verses = self._parse_verse_numbers(verses_str)
                    current_content = []
                    pattern_type = "equals"
                    continue
                
                # íŒ¨í„´ 2: ë…ë¦½ëœ ì¤„ì˜ ì ˆ ë²ˆí˜¸
                line_match = re.match(line_pattern, line)
                if line_match:
                    # ì´ì „ ì ˆ ì €ì¥
                    if current_verses and current_content:
                        content_text = '\n'.join(current_content).strip()
                        if content_text:
                            self._add_verses_data(verses_data, article_id, url, title, commentary_name, 
                                                book_name, chapter, current_verses, content_text, "lines")
                    
                    # ìƒˆ ì ˆ ì‹œì‘
                    ch = int(line_match.group(1))
                    verses_str = line_match.group(2)
                    current_verses = self._parse_verse_numbers(verses_str)
                    current_content = []
                    pattern_type = "lines"
                    continue
                
                # ë‚´ìš© ë¼ì¸
                if current_verses:
                    current_content.append(line)
            
            # ë§ˆì§€ë§‰ ì ˆ ì €ì¥
            if current_verses and current_content:
                content_text = '\n'.join(current_content).strip()
                if content_text:
                    self._add_verses_data(verses_data, article_id, url, title, commentary_name, 
                                        book_name, chapter, current_verses, content_text, pattern_type)
            
            # ì ˆì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ ë‚´ìš©ì„ 1ì ˆë¡œ ì €ì¥
            if not verses_data and lines:
                full_content = '\n'.join(lines).strip()
                if full_content:
                    verses_data.append({
                        'article_id': article_id,
                        'url': url,
                        'title': title,
                        'commentary_name': commentary_name,
                        'book_name': book_name,
                        'chapter': chapter,
                        'verse': 1,
                        'content': full_content,
                        'content_length': len(full_content),
                        'pattern_type': 'none'
                    })
            
            return verses_data
            
        except Exception as e:
            print(f"âŒ ê²Œì‹œê¸€ {article_id} íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    def _parse_verse_numbers(self, verses_str):
        """ì ˆ ë²ˆí˜¸ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì ˆ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        verses = []
        
        for part in verses_str.split(','):
            part = part.strip()
            if '-' in part:
                # ë²”ìœ„: 33-35 -> [33, 34, 35]
                start, end = map(int, part.split('-'))
                verses.extend(range(start, end + 1))
            else:
                # ë‹¨ì¼ ì ˆ
                verses.append(int(part))
        
        return verses

    def _add_verses_data(self, verses_data, article_id, url, title, commentary_name, 
                        book_name, chapter, verses, content, pattern_type):
        """ì ˆ ë°ì´í„°ë¥¼ ì¶”ê°€"""
        for verse in verses:
            verses_data.append({
                'article_id': article_id,
                'url': url,
                'title': title,
                'commentary_name': commentary_name,
                'book_name': book_name,
                'chapter': chapter,
                'verse': verse,
                'content': content,
                'content_length': len(content),
                'pattern_type': pattern_type
            })

    def parse_multiple_articles(self, article_ids, progress_callback=None):
        """ì—¬ëŸ¬ ê²Œì‹œê¸€ íŒŒì‹±"""
        all_data = []
        failed_articles = []
        
        print(f"ğŸš€ {len(article_ids)}ê°œ ê²Œì‹œê¸€ íŒŒì‹± ì‹œì‘...")
        start_time = time.time()
        
        for i, article_id in enumerate(article_ids):
            try:
                if progress_callback and i % 10 == 0:
                    progress_callback(i, len(article_ids))
                
                verses_data = self.parse_article_content(article_id)
                
                if verses_data:
                    all_data.extend(verses_data)
                    if i % 50 == 0:  # 50ê°œë§ˆë‹¤ ì¶œë ¥
                        print(f"  ì§„í–‰: {i+1}/{len(article_ids)} - ê²Œì‹œê¸€ {article_id}: {len(verses_data)}ê°œ ì ˆ")
                else:
                    failed_articles.append(article_id)
                    print(f"  âŒ ê²Œì‹œê¸€ {article_id}: íŒŒì‹± ì‹¤íŒ¨")
                
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ ê²Œì‹œê¸€ {article_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                failed_articles.append(article_id)
        
        elapsed_time = time.time() - start_time
        print(f"\nâœ… íŒŒì‹± ì™„ë£Œ!")
        print(f"  ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        print(f"  ì„±ê³µ: {len(article_ids) - len(failed_articles)}/{len(article_ids)}")
        print(f"  ì´ ì ˆ ìˆ˜: {len(all_data):,}")
        print(f"  ì‹¤íŒ¨í•œ ê²Œì‹œê¸€: {len(failed_articles)}ê°œ")
        
        return all_data, failed_articles

    def save_to_excel(self, data, filename=None):
        """ì—‘ì…€ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"corrected_hochma_commentaries_{timestamp}.xlsx"
        
        if not data:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ’¾ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ ì¤‘: {filename}")
        
        df = pd.DataFrame(data)
        
        # í†µê³„ ìƒì„±
        stats_data = []
        
        # ì„±ê²½ì±…ë³„ í†µê³„
        book_stats = df.groupby('book_name').agg({
            'article_id': 'nunique',
            'chapter': ['nunique', 'min', 'max'],
            'verse': 'count',
            'content_length': 'sum'
        }).round(2)
        
        for book in book_stats.index:
            if pd.notna(book):
                stats_data.append({
                    'ì„±ê²½ì±…': book,
                    'ê²Œì‹œê¸€ìˆ˜': book_stats.loc[book, ('article_id', 'nunique')],
                    'ì¥ìˆ˜': book_stats.loc[book, ('chapter', 'nunique')],
                    'ìµœì†Œì¥': book_stats.loc[book, ('chapter', 'min')],
                    'ìµœëŒ€ì¥': book_stats.loc[book, ('chapter', 'max')],
                    'ì ˆìˆ˜': book_stats.loc[book, ('verse', 'count')],
                    'ì´ê¸€ììˆ˜': book_stats.loc[book, ('content_length', 'sum')]
                })
        
        stats_df = pd.DataFrame(stats_data)
        
        # ìš”ì•½ ì •ë³´
        summary_data = [{
            'êµ¬ë¶„': 'ì „ì²´ í†µê³„',
            'ê°’': f"ê²Œì‹œê¸€ {df['article_id'].nunique()}ê°œ, ì„±ê²½ì±… {df['book_name'].nunique()}ê°œ, ì ˆ {len(df):,}ê°œ"
        }, {
            'êµ¬ë¶„': 'ì´ ê¸€ììˆ˜',
            'ê°’': f"{df['content_length'].sum():,}ì"
        }, {
            'êµ¬ë¶„': 'í‰ê·  ì ˆ ê¸¸ì´',
            'ê°’': f"{df['content_length'].mean():.0f}ì"
        }]
        
        summary_df = pd.DataFrame(summary_data)
        
        # ì—‘ì…€ ì €ì¥
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='í˜¸í¬ë§ˆ ì£¼ì„ ë°ì´í„°', index=False)
            summary_df.to_excel(writer, sheet_name='ìš”ì•½ ì •ë³´', index=False)
            stats_df.to_excel(writer, sheet_name='ì„±ê²½ì±…ë³„ í†µê³„', index=False)
        
        import os
        file_size = os.path.getsize(filename)
        file_size_mb = file_size / (1024 * 1024)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename} ({file_size_mb:.1f}MB)")
        
        return filename

def load_article_ids():
    """ì´ì „ì— ë°œê²¬í•œ ê²Œì‹œê¸€ ID ëª©ë¡ ë¡œë“œ"""
    try:
        with open('found_articles.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('article_ids', [])
    except:
        # ê¸°ë³¸ ë²”ìœ„ë¡œ ì‹œë„
        return list(range(139393, 141927))

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”§ í˜¸í¬ë§ˆ ì£¼ì„ ì˜¬ë°”ë¥¸ íŒŒì‹± ì‹œì‘")
    print("=" * 50)
    
    # ê²Œì‹œê¸€ ID ëª©ë¡ ë¡œë“œ
    article_ids = load_article_ids()
    print(f"ğŸ“‹ íŒŒì‹±í•  ê²Œì‹œê¸€ ìˆ˜: {len(article_ids)}ê°œ")
    
    # íŒŒì„œ ìƒì„±
    parser = CorrectedHochmaParser()
    
    # ì§„í–‰ ìƒí™© ì½œë°±
    def progress_callback(current, total):
        percent = (current / total) * 100
        print(f"  ì§„í–‰ë¥ : {percent:.1f}% ({current}/{total})")
    
    # íŒŒì‹± ì‹¤í–‰
    data, failed = parser.parse_multiple_articles(article_ids, progress_callback)
    
    if data:
        # ì—‘ì…€ ì €ì¥
        filename = parser.save_to_excel(data)
        
        # ê²°ê³¼ ë¶„ì„
        df = pd.DataFrame(data)
        print(f"\nğŸ“Š íŒŒì‹± ê²°ê³¼ ë¶„ì„:")
        print(f"  ì´ ì ˆ ìˆ˜: {len(df):,}")
        print(f"  ì„±ê²½ì±… ìˆ˜: {df['book_name'].nunique()}")
        print(f"  ê²Œì‹œê¸€ ìˆ˜: {df['article_id'].nunique()}")
        
        # ì„±ê²½ì±…ë³„ ìš”ì•½
        print(f"\nğŸ“š ì„±ê²½ì±…ë³„ ìš”ì•½:")
        book_summary = df.groupby('book_name').agg({
            'chapter': 'nunique',
            'verse': 'count'
        }).sort_values('chapter', ascending=False)
        
        for book, stats in book_summary.head(10).iterrows():
            print(f"  {book}: {stats['chapter']}ì¥, {stats['verse']}ì ˆ")
    
    else:
        print("âŒ íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 