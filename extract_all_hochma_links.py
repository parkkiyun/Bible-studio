import requests
from bs4 import BeautifulSoup
import re
import json
import time
from datetime import datetime
import pandas as pd

class HochmaLinkExtractor:
    def __init__(self):
        self.base_url = "https://nocr.net/com_kor_hochma"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def extract_links_from_page(self, page_num=1):
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ëª¨ë“  ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ"""
        if page_num == 1:
            url = self.base_url
        else:
            url = f"{self.base_url}?page={page_num}"
        
        print(f"ğŸ“„ í˜ì´ì§€ {page_num} ìŠ¤ìº” ì¤‘: {url}")
        
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"âŒ í˜ì´ì§€ {page_num} ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
                return [], False
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° - ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
            links = []
            
            # íŒ¨í„´ 1: /com_kor_hochma/ìˆ«ì í˜•íƒœì˜ ë§í¬
            pattern1_links = soup.find_all('a', href=re.compile(r'/com_kor_hochma/(\d+)'))
            for link in pattern1_links:
                href = link.get('href')
                match = re.search(r'/com_kor_hochma/(\d+)', href)
                if match:
                    article_id = int(match.group(1))
                    title = link.get_text(strip=True)
                    links.append({
                        'article_id': article_id,
                        'title': title,
                        'href': href,
                        'page': page_num
                    })
            
            # íŒ¨í„´ 2: document_srl= íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ë§í¬
            pattern2_links = soup.find_all('a', href=re.compile(r'document_srl=(\d+)'))
            for link in pattern2_links:
                href = link.get('href')
                match = re.search(r'document_srl=(\d+)', href)
                if match:
                    article_id = int(match.group(1))
                    title = link.get_text(strip=True)
                    # ì¤‘ë³µ ì œê±°
                    if not any(l['article_id'] == article_id for l in links):
                        links.append({
                            'article_id': article_id,
                            'title': title,
                            'href': href,
                            'page': page_num
                        })
            
            # í˜¸í¬ë§ˆ ì£¼ì„ íŒ¨í„´ì´ ìˆëŠ” ë§í¬ë§Œ í•„í„°ë§
            filtered_links = []
            for link in links:
                if 'í˜¸í¬ë§ˆ ì£¼ì„' in link['title']:
                    filtered_links.append(link)
            
            print(f"  ë°œê²¬ëœ ë§í¬: {len(links)}ê°œ, í˜¸í¬ë§ˆ ì£¼ì„: {len(filtered_links)}ê°œ")
            
            # ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ í™•ì¸
            has_next = self.check_next_page_exists(soup, page_num)
            
            return filtered_links, has_next
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return [], False
    
    def check_next_page_exists(self, soup, current_page):
        """ë‹¤ìŒ í˜ì´ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            # í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ ì°¾ê¸°
            pagination = soup.find('div', class_='pagination') or soup.find('div', class_='paging')
            
            if pagination:
                # "Next" ë²„íŠ¼ì´ë‚˜ ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì°¾ê¸°
                next_links = pagination.find_all('a', href=re.compile(f'page={current_page + 1}'))
                if next_links:
                    return True
                
                # ë˜ëŠ” ìˆ«ìë¡œ ëœ í˜ì´ì§€ ë§í¬ í™•ì¸
                page_links = pagination.find_all('a', href=re.compile(r'page=(\d+)'))
                max_page = 0
                for link in page_links:
                    match = re.search(r'page=(\d+)', link.get('href', ''))
                    if match:
                        page_num = int(match.group(1))
                        max_page = max(max_page, page_num)
                
                return current_page < max_page
            
            # í…Œì´ë¸” ê¸°ë°˜ ëª©ë¡ì—ì„œ í–‰ ìˆ˜ í™•ì¸ (í˜ì´ì§€ë‹¹ ë³´í†µ 50ê°œ)
            table_rows = soup.find_all('tr')
            article_rows = [row for row in table_rows if row.find('a', href=re.compile(r'com_kor_hochma'))]
            
            # 50ê°œ í–‰ì´ ìˆìœ¼ë©´ ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆì„ ê°€ëŠ¥ì„±
            return len(article_rows) >= 50
            
        except Exception as e:
            print(f"âš ï¸ ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def extract_all_links(self, max_pages=100):
        """ëª¨ë“  í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ"""
        print("ğŸ” í˜¸í¬ë§ˆ ì‚¬ì´íŠ¸ ëª¨ë“  ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ ì‹œì‘")
        print("=" * 60)
        
        all_links = []
        page_num = 1
        
        while page_num <= max_pages:
            links, has_next = self.extract_links_from_page(page_num)
            
            if links:
                all_links.extend(links)
                print(f"  âœ… í˜ì´ì§€ {page_num}: {len(links)}ê°œ ë§í¬ ì¶”ê°€")
            else:
                print(f"  âš ï¸ í˜ì´ì§€ {page_num}: ë§í¬ ì—†ìŒ")
            
            if not has_next:
                print(f"  ğŸ í˜ì´ì§€ {page_num}ì—ì„œ ì¢…ë£Œ (ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ)")
                break
            
            page_num += 1
            time.sleep(0.5)  # ìš”ì²­ ê°„ê²©
        
        # ì¤‘ë³µ ì œê±° (article_id ê¸°ì¤€)
        unique_links = {}
        for link in all_links:
            article_id = link['article_id']
            if article_id not in unique_links:
                unique_links[article_id] = link
        
        unique_links_list = list(unique_links.values())
        unique_links_list.sort(key=lambda x: x['article_id'])
        
        print(f"\nğŸ“Š ì¶”ì¶œ ê²°ê³¼:")
        print(f"  ì´ ìŠ¤ìº” í˜ì´ì§€: {page_num - 1}")
        print(f"  ë°œê²¬ëœ ì „ì²´ ë§í¬: {len(all_links)}")
        print(f"  ì¤‘ë³µ ì œê±° í›„: {len(unique_links_list)}")
        print(f"  article_id ë²”ìœ„: {min(unique_links_list, key=lambda x: x['article_id'])['article_id']} ~ {max(unique_links_list, key=lambda x: x['article_id'])['article_id']}")
        
        return unique_links_list
    
    def analyze_extracted_links(self, links):
        """ì¶”ì¶œëœ ë§í¬ë“¤ ë¶„ì„"""
        print(f"\nğŸ“Š ì¶”ì¶œëœ ë§í¬ ë¶„ì„")
        print("=" * 40)
        
        # ì„±ê²½ì±…ë³„ ë¶„ë¥˜
        book_articles = {}
        unmatched_titles = []
        
        for link in links:
            title = link['title']
            
            # í˜¸í¬ë§ˆ ì£¼ì„ íŒ¨í„´ì—ì„œ ì„±ê²½ì±…ëª…ê³¼ ì¥ ì¶”ì¶œ
            pattern = r'í˜¸í¬ë§ˆ ì£¼ì„[,\s]*([ê°€-í£]+(?:ìƒ|í•˜)?(?:ì „ì„œ|í›„ì„œ)?(?:ì¼ì„œ|ì´ì„œ|ì‚¼ì„œ)?(?:ë³µìŒ)?(?:ê¸°)?(?:ì• ê°€)?)\s*(\d+)ì¥'
            match = re.search(pattern, title)
            
            if match:
                book_name = match.group(1)
                chapter = int(match.group(2))
                
                if book_name not in book_articles:
                    book_articles[book_name] = []
                
                book_articles[book_name].append({
                    'article_id': link['article_id'],
                    'chapter': chapter,
                    'title': title
                })
            else:
                unmatched_titles.append(title)
        
        print(f"ğŸ“š ì„±ê²½ì±…ë³„ í†µê³„:")
        for book_name in sorted(book_articles.keys()):
            chapters = sorted([item['chapter'] for item in book_articles[book_name]])
            article_count = len(book_articles[book_name])
            min_chapter = min(chapters)
            max_chapter = max(chapters)
            
            print(f"  {book_name}: {article_count}ê°œ ({min_chapter}~{max_chapter}ì¥)")
        
        if unmatched_titles:
            print(f"\nâš ï¸ íŒ¨í„´ ë§¤ì¹˜ ì•ˆëœ ì œëª©ë“¤ ({len(unmatched_titles)}ê°œ):")
            for title in unmatched_titles[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                print(f"  - {title}")
        
        return book_articles
    
    def save_results(self, links, book_articles):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ì „ì²´ ë§í¬ ëª©ë¡ ì €ì¥
        links_df = pd.DataFrame(links)
        links_filename = f"hochma_all_links_{timestamp}.xlsx"
        links_df.to_excel(links_filename, index=False)
        
        # 2. ì„±ê²½ì±…ë³„ ì •ë¦¬ëœ ë°ì´í„° ì €ì¥
        organized_data = []
        for book_name, articles in book_articles.items():
            for article in articles:
                organized_data.append({
                    'book_name': book_name,
                    'chapter': article['chapter'],
                    'article_id': article['article_id'],
                    'title': article['title']
                })
        
        organized_df = pd.DataFrame(organized_data)
        organized_df = organized_df.sort_values(['book_name', 'chapter'])
        organized_filename = f"hochma_organized_links_{timestamp}.xlsx"
        organized_df.to_excel(organized_filename, index=False)
        
        # 3. JSON í˜•íƒœë¡œë„ ì €ì¥
        json_data = {
            'extraction_timestamp': timestamp,
            'total_links': len(links),
            'total_books': len(book_articles),
            'links': links,
            'organized_by_book': book_articles
        }
        
        json_filename = f"hochma_all_links_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥:")
        print(f"  ì „ì²´ ë§í¬: {links_filename}")
        print(f"  ì •ë¦¬ëœ ë°ì´í„°: {organized_filename}")
        print(f"  JSON ë°ì´í„°: {json_filename}")
        
        return links_filename, organized_filename, json_filename

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”— í˜¸í¬ë§ˆ ì‚¬ì´íŠ¸ ëª¨ë“  ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ")
    print("=" * 50)
    
    extractor = HochmaLinkExtractor()
    
    # 1ë‹¨ê³„: ëª¨ë“  ë§í¬ ì¶”ì¶œ
    links = extractor.extract_all_links(max_pages=50)  # ìµœëŒ€ 50í˜ì´ì§€ê¹Œì§€
    
    if not links:
        print("âŒ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2ë‹¨ê³„: ì¶”ì¶œëœ ë§í¬ ë¶„ì„
    book_articles = extractor.analyze_extracted_links(links)
    
    # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    links_file, organized_file, json_file = extractor.save_results(links, book_articles)
    
    print(f"\nâœ… ë§í¬ ì¶”ì¶œ ì™„ë£Œ!")
    print(f"  ë°œê²¬ëœ ê²Œì‹œê¸€: {len(links)}ê°œ")
    print(f"  ë°œê²¬ëœ ì„±ê²½ì±…: {len(book_articles)}ê°œ")
    
    # 4ë‹¨ê³„: Bible Databaseì™€ ë¹„êµ
    print(f"\nğŸ“– ì™„ì„±ë„ ì˜ˆìƒ:")
    expected_total = 1189  # ì‚¬ì´íŠ¸ì—ì„œ í‘œì‹œëœ ì´ ê²Œì‹œê¸€ ìˆ˜
    actual_found = len(links)
    completion_rate = (actual_found / expected_total) * 100
    
    print(f"  ì˜ˆìƒ ì´ ê²Œì‹œê¸€: {expected_total}ê°œ")
    print(f"  ì‹¤ì œ ë°œê²¬: {actual_found}ê°œ")
    print(f"  ì™„ì„±ë„: {completion_rate:.1f}%")

if __name__ == "__main__":
    main() 