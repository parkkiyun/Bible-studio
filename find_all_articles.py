import requests
from bs4 import BeautifulSoup
import re
import time
import json
from datetime import datetime

def find_all_article_ids():
    """í˜¸í¬ë§ˆ ì‚¬ì´íŠ¸ì—ì„œ ëª¨ë“  6ìë¦¬ ê²Œì‹œê¸€ ID ì°¾ê¸°"""
    
    base_url = "https://nocr.net"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    print("ğŸ” í˜¸í¬ë§ˆ ì‚¬ì´íŠ¸ì—ì„œ ê²Œì‹œê¸€ ID ìˆ˜ì§‘ ì¤‘...")
    
    all_ids = set()
    
    # ë°©ë²• 1: ë²”ìœ„ ê¸°ë°˜ ìŠ¤ìº” (ê¸°ì¡´ì— ë³¸ IDë“¤ ê¸°ì¤€)
    print("\nğŸ“Š ë²”ìœ„ ê¸°ë°˜ ìŠ¤ìº”...")
    known_ids = [139393, 139453, 139475, 139477]  # ì•Œë ¤ì§„ IDë“¤
    
    # ìµœì†Œ/ìµœëŒ€ê°’ ì¶”ì •
    min_id = min(known_ids) - 100
    max_id = max(known_ids) + 100
    
    print(f"ìŠ¤ìº” ë²”ìœ„: {min_id} ~ {max_id}")
    
    valid_ids = []
    failed_count = 0
    
    for article_id in range(min_id, max_id + 1):
        url = f"{base_url}/com_kor_hochma/{article_id}"
        
        try:
            response = requests.get(url, headers=headers, timeout=5)
            
            if response.status_code == 200:
                # ì‹¤ì œ í˜¸í¬ë§ˆ ì£¼ì„ í˜ì´ì§€ì¸ì§€ í™•ì¸
                soup = BeautifulSoup(response.text, 'html.parser')
                title_tag = soup.find('title')
                
                if title_tag:
                    title_text = title_tag.get_text()
                    if 'ì£¼ì„' in title_text and any(book in title_text for book in 
                        ['ì°½ì„¸ê¸°', 'ì¶œì• êµ½ê¸°', 'ë ˆìœ„ê¸°', 'ë¯¼ìˆ˜ê¸°', 'ì‹ ëª…ê¸°', 'ì—¬í˜¸ìˆ˜ì•„', 'ì‚¬ì‚¬ê¸°', 
                         'ë£»ê¸°', 'ì‚¬ë¬´ì—˜', 'ì—´ì™•ê¸°', 'ì—­ëŒ€', 'ì—ìŠ¤ë¼', 'ëŠí—¤ë¯¸ì•¼', 'ì—ìŠ¤ë”', 
                         'ìš¥ê¸°', 'ì‹œí¸', 'ì ì–¸', 'ì „ë„ì„œ', 'ì•„ê°€', 'ì´ì‚¬ì•¼', 'ì˜ˆë ˆë¯¸ì•¼', 'ì—ìŠ¤ê²”', 
                         'ë‹¤ë‹ˆì—˜', 'í˜¸ì„¸ì•„', 'ìš”ì—˜', 'ì•„ëª¨ìŠ¤', 'ì˜¤ë°”ëŒœ', 'ìš”ë‚˜', 'ë¯¸ê°€', 'ë‚˜í›”', 
                         'í•˜ë°•êµ­', 'ìŠ¤ë°”ëƒ', 'í•™ê°œ', 'ìŠ¤ê°€ë´', 'ë§ë¼ê¸°', 'ë§ˆíƒœ', 'ë§ˆê°€', 'ëˆ„ê°€', 
                         'ìš”í•œ', 'ì‚¬ë„í–‰ì „', 'ë¡œë§ˆì„œ', 'ê³ ë¦°ë„', 'ê°ˆë¼ë””ì•„', 'ì—ë² ì†Œ', 'ë¹Œë¦½ë³´', 
                         'ê³¨ë¡œìƒˆ', 'ë°ì‚´ë¡œë‹ˆê°€', 'ë””ëª¨ë°', 'ë””ë„', 'ë¹Œë ˆëª¬', 'íˆë¸Œë¦¬ì„œ', 'ì•¼ê³ ë³´', 
                         'ë² ë“œë¡œ', 'ìœ ë‹¤', 'ê³„ì‹œë¡']):
                        valid_ids.append(article_id)
                        all_ids.add(article_id)
                        print(f"âœ“ {article_id}: {title_text[:50]}...")
                        failed_count = 0
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
            else:
                failed_count += 1
                
        except Exception as e:
            failed_count += 1
            
        # ì—°ì† ì‹¤íŒ¨ ì‹œ ë²”ìœ„ í™•ì¥ ì¤‘ë‹¨
        if failed_count > 20:
            print(f"  ì—°ì† ì‹¤íŒ¨ 20íšŒ, ë²”ìœ„ ìŠ¤ìº” ì¤‘ë‹¨")
            break
            
        # ì„œë²„ ë¶€í•˜ ë°©ì§€
        time.sleep(0.1)
        
        if article_id % 50 == 0:
            print(f"  ì§„í–‰: {article_id} (ë°œê²¬: {len(valid_ids)}ê°œ)")
    
    print(f"\nğŸ“‹ ë²”ìœ„ ìŠ¤ìº” ê²°ê³¼: {len(valid_ids)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
    
    # ë°©ë²• 2: ë” ë„“ì€ ë²”ìœ„ë¡œ ìƒ˜í”Œë§
    print(f"\nğŸ¯ í™•ì¥ ìƒ˜í”Œë§...")
    
    # 6ìë¦¬ ID ë²”ìœ„ì—ì„œ ìƒ˜í”Œë§ (100000~999999)
    sample_ranges = [
        (100000, 110000, 1000),  # 10ë§ŒëŒ€, 1000ê°œì”© ì í”„
        (130000, 150000, 100),   # 13-15ë§ŒëŒ€, 100ê°œì”© ì í”„ (ì•Œë ¤ì§„ ë²”ìœ„ ê·¼ì²˜)
        (200000, 210000, 1000),  # 20ë§ŒëŒ€
        (500000, 510000, 1000),  # 50ë§ŒëŒ€
    ]
    
    for start, end, step in sample_ranges:
        print(f"  ìƒ˜í”Œë§: {start}~{end} (step={step})")
        sample_count = 0
        
        for article_id in range(start, end, step):
            url = f"{base_url}/com_kor_hochma/{article_id}"
            
            try:
                response = requests.get(url, headers=headers, timeout=3)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    title_tag = soup.find('title')
                    
                    if title_tag and 'ì£¼ì„' in title_tag.get_text():
                        if article_id not in all_ids:
                            all_ids.add(article_id)
                            print(f"    âœ“ {article_id}: ìƒˆ ê²Œì‹œê¸€ ë°œê²¬")
                            sample_count += 1
                            
            except:
                pass
                
            time.sleep(0.05)  # ë¹ ë¥¸ ìƒ˜í”Œë§
            
        print(f"    â†’ {sample_count}ê°œ ì¶”ê°€ ë°œê²¬")
    
    # ê²°ê³¼ ì •ë¦¬
    final_ids = sorted(list(all_ids))
    
    print(f"\nğŸ‰ ìµœì¢… ê²°ê³¼:")
    print(f"  ì´ {len(final_ids)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
    print(f"  ID ë²”ìœ„: {min(final_ids)} ~ {max(final_ids)}")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    result = {
        'scan_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_count': len(final_ids),
        'article_ids': final_ids,
        'id_range': {
            'min': min(final_ids),
            'max': max(final_ids)
        }
    }
    
    json_filename = f"hochma_article_ids_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"  ì €ì¥: {json_filename}")
    
    return final_ids

if __name__ == "__main__":
    article_ids = find_all_article_ids() 