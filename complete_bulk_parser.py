import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import time
import json
from datetime import datetime
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
import os

class CompleteBulkHochmaParser:
    def __init__(self):
        self.base_url = "https://nocr.net/com_kor_hochma"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.results = []
        self.failed_ids = []
        self.stats = {
            'total_articles': 0,
            'successful_parses': 0,
            'failed_parses': 0,
            'total_verses': 0,
            'start_time': None,
            'end_time': None
        }
    
    def detect_verse_pattern(self, content):
        """ê²Œì‹œê¸€ì˜ ì ˆ êµ¬ë¶„ íŒ¨í„´ ê°ì§€"""
        
        # íŒ¨í„´ 1: ====31:1 ë˜ëŠ” ===31:1 í˜•íƒœ
        equals_pattern = re.findall(r'={3,}(\d+):(\d+)', content)
        if equals_pattern:
            return 'equals', equals_pattern
        
        # íŒ¨í„´ 2: ì¤„ë°”ê¿ˆ ê¸°ë°˜ 31:1 í˜•íƒœ
        lines = content.split('\n')
        line_pattern = []
        for i, line in enumerate(lines):
            line = line.strip()
            # ì ˆ ë²ˆí˜¸ íŒ¨í„´: ìˆ«ì:ìˆ«ì í˜•íƒœê°€ ë…ë¦½ì ì¸ ì¤„ì— ìˆëŠ” ê²½ìš°
            if re.match(r'^\d+:\d+([,-]\d+)*$', line):
                line_pattern.append((line, i))
        
        if line_pattern:
            return 'lines', line_pattern
        
        return 'none', []
    
    def parse_verse_reference(self, verse_ref):
        """ì ˆ ì°¸ì¡°ë¥¼ íŒŒì‹±í•´ì„œ ê°œë³„ ì ˆ ë²ˆí˜¸ë“¤ë¡œ ë³€í™˜"""
        verses = []
        
        # ì½¤ë§ˆë¡œ ë¶„ë¦¬ëœ ê²½ìš°: 19:37,38
        if ',' in verse_ref:
            parts = verse_ref.split(',')
            chapter = parts[0].split(':')[0]
            for part in parts:
                if ':' in part:
                    verses.append(part.strip())
                else:
                    verses.append(f"{chapter}:{part.strip()}")
        
        # í•˜ì´í”ˆìœ¼ë¡œ ë²”ìœ„ ì§€ì •ëœ ê²½ìš°: 19:33-35
        elif '-' in verse_ref and ':' in verse_ref:
            chapter_verse, end_verse = verse_ref.split('-')
            chapter, start_verse = chapter_verse.split(':')
            start_verse = int(start_verse)
            end_verse = int(end_verse.strip())
            
            for v in range(start_verse, end_verse + 1):
                verses.append(f"{chapter}:{v}")
        
        # ë‹¨ì¼ ì ˆ: 19:1
        else:
            verses.append(verse_ref.strip())
        
        return verses
    
    def parse_single_article(self, article_id):
        """ë‹¨ì¼ ê²Œì‹œê¸€ íŒŒì‹±"""
        url = f"{self.base_url}/{article_id}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = ""
            title_tag = soup.find('h1')
            if title_tag:
                title = title_tag.get_text(strip=True)
            else:
                title_tag = soup.find('title')
                if title_tag:
                    title = title_tag.get_text(strip=True)
            
            # ì œëª©ì—ì„œ ì •ë³´ ì¶”ì¶œ
            commentary_name = ""
            book_name = ""
            chapter = ""
            
            if title:
                # ì£¼ì„ ì´ë¦„ ì¶”ì¶œ
                if "ì£¼ì„" in title:
                    commentary_match = re.search(r'(\w+)\s*ì£¼ì„', title)
                    if commentary_match:
                        commentary_name = commentary_match.group(1) + " ì£¼ì„"
                
                # ì„±ê²½ì±… ì´ë¦„ê³¼ ì¥ ì¶”ì¶œ
                bible_books = ['ì°½ì„¸ê¸°', 'ì¶œì• êµ½ê¸°', 'ë ˆìœ„ê¸°', 'ë¯¼ìˆ˜ê¸°', 'ì‹ ëª…ê¸°', 'ì—¬í˜¸ìˆ˜ì•„', 'ì‚¬ì‚¬ê¸°', 
                              'ë£»ê¸°', 'ì‚¬ë¬´ì—˜ìƒ', 'ì‚¬ë¬´ì—˜í•˜', 'ì—´ì™•ê¸°ìƒ', 'ì—´ì™•ê¸°í•˜', 'ì—­ëŒ€ìƒ', 'ì—­ëŒ€í•˜', 
                              'ì—ìŠ¤ë¼', 'ëŠí—¤ë¯¸ì•¼', 'ì—ìŠ¤ë”', 'ìš¥ê¸°', 'ì‹œí¸', 'ì ì–¸', 'ì „ë„ì„œ', 'ì•„ê°€', 
                              'ì´ì‚¬ì•¼', 'ì˜ˆë ˆë¯¸ì•¼', 'ì˜ˆë ˆë¯¸ì•¼ì• ê°€', 'ì—ìŠ¤ê²”', 'ë‹¤ë‹ˆì—˜', 'í˜¸ì„¸ì•„', 'ìš”ì—˜', 
                              'ì•„ëª¨ìŠ¤', 'ì˜¤ë°”ëŒœ', 'ìš”ë‚˜', 'ë¯¸ê°€', 'ë‚˜í›”', 'í•˜ë°•êµ­', 'ìŠ¤ë°”ëƒ', 'í•™ê°œ', 
                              'ìŠ¤ê°€ë´', 'ë§ë¼ê¸°', 'ë§ˆíƒœë³µìŒ', 'ë§ˆê°€ë³µìŒ', 'ëˆ„ê°€ë³µìŒ', 'ìš”í•œë³µìŒ', 
                              'ì‚¬ë„í–‰ì „', 'ë¡œë§ˆì„œ', 'ê³ ë¦°ë„ì „ì„œ', 'ê³ ë¦°ë„í›„ì„œ', 'ê°ˆë¼ë””ì•„ì„œ', 'ì—ë² ì†Œì„œ', 
                              'ë¹Œë¦½ë³´ì„œ', 'ê³¨ë¡œìƒˆì„œ', 'ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ', 'ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ', 'ë””ëª¨ë°ì „ì„œ', 
                              'ë””ëª¨ë°í›„ì„œ', 'ë””ë„ì„œ', 'ë¹Œë ˆëª¬ì„œ', 'íˆë¸Œë¦¬ì„œ', 'ì•¼ê³ ë³´ì„œ', 'ë² ë“œë¡œì „ì„œ', 
                              'ë² ë“œë¡œí›„ì„œ', 'ìš”í•œ1ì„œ', 'ìš”í•œ2ì„œ', 'ìš”í•œ3ì„œ', 'ìœ ë‹¤ì„œ', 'ìš”í•œê³„ì‹œë¡',
                              'ìš”í•œì¼ì„œ', 'ìš”í•œì´ì„œ', 'ìš”í•œì‚¼ì„œ']
                
                for book in bible_books:
                    if book in title:
                        book_name = book
                        # ì¥ ë²ˆí˜¸ ì¶”ì¶œ
                        chapter_match = re.search(f'{book}\\s*(\\d+)ì¥', title)
                        if chapter_match:
                            chapter = chapter_match.group(1)
                        break
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (BR íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜)
            content = ""
            content_selectors = ['.xe_content', '.rd_body', '.rhymix_content', '.document_content']
            
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    # <br> íƒœê·¸ë¥¼ \nìœ¼ë¡œ ë³€í™˜
                    for br in content_div.find_all('br'):
                        br.replace_with('\n')
                    content = content_div.get_text()
                    break
            
            if not content:
                print(f"âš ï¸  {article_id}: ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            # ì ˆ íŒ¨í„´ ê°ì§€ ë° íŒŒì‹±
            pattern_type, pattern_data = self.detect_verse_pattern(content)
            
            parsed_verses = []
            
            if pattern_type == 'equals':
                # === ë˜ëŠ” ==== íŒ¨í„´
                content_parts = re.split(r'={3,}\d+:\d+', content)
                
                for i, (chapter_num, verse_num) in enumerate(pattern_data):
                    if i + 1 < len(content_parts):
                        verse_content = content_parts[i + 1].strip()
                        if verse_content:
                            parsed_verses.append({
                                'article_id': article_id,
                                'url': url,
                                'title': title,
                                'commentary_name': commentary_name,
                                'book_name': book_name,
                                'chapter': int(chapter_num),
                                'verse': int(verse_num),
                                'content': verse_content,
                                'content_length': len(verse_content),
                                'pattern_type': 'equals'
                            })
            
            elif pattern_type == 'lines':
                # ì¤„ë°”ê¿ˆ ê¸°ë°˜ íŒ¨í„´
                lines = content.split('\n')
                
                for i, (verse_ref, line_idx) in enumerate(pattern_data):
                    # ë‹¤ìŒ ì ˆ ì°¸ì¡°ê¹Œì§€ì˜ ë‚´ìš© ì¶”ì¶œ
                    start_idx = line_idx + 1
                    end_idx = len(lines)
                    
                    if i + 1 < len(pattern_data):
                        end_idx = pattern_data[i + 1][1]
                    
                    verse_content = '\n'.join(lines[start_idx:end_idx]).strip()
                    
                    if verse_content:
                        # ì ˆ ì°¸ì¡° íŒŒì‹± (ì½¤ë§ˆ, í•˜ì´í”ˆ ì²˜ë¦¬)
                        individual_verses = self.parse_verse_reference(verse_ref)
                        
                        for verse_str in individual_verses:
                            if ':' in verse_str:
                                chapter_num, verse_num = verse_str.split(':')
                                parsed_verses.append({
                                    'article_id': article_id,
                                    'url': url,
                                    'title': title,
                                    'commentary_name': commentary_name,
                                    'book_name': book_name,
                                    'chapter': int(chapter_num),
                                    'verse': int(verse_num),
                                    'content': verse_content,
                                    'content_length': len(verse_content),
                                    'pattern_type': 'lines'
                                })
            
            else:
                # íŒ¨í„´ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í•­ëª©ìœ¼ë¡œ
                parsed_verses.append({
                    'article_id': article_id,
                    'url': url,
                    'title': title,
                    'commentary_name': commentary_name,
                    'book_name': book_name,
                    'chapter': int(chapter) if chapter else 0,
                    'verse': 0,
                    'content': content.strip(),
                    'content_length': len(content.strip()),
                    'pattern_type': 'none'
                })
            
            print(f"âœ… {article_id}: {len(parsed_verses)}ê°œ ì ˆ íŒŒì‹± ì™„ë£Œ ({pattern_type} íŒ¨í„´)")
            return parsed_verses
            
        except Exception as e:
            print(f"âŒ {article_id}: íŒŒì‹± ì‹¤íŒ¨ - {str(e)}")
            self.failed_ids.append(article_id)
            return []
    
    def parse_all_articles(self, article_ids):
        """ëª¨ë“  ê²Œì‹œê¸€ íŒŒì‹±"""
        print(f"ğŸš€ {len(article_ids)}ê°œ ê²Œì‹œê¸€ ëŒ€ëŸ‰ íŒŒì‹± ì‹œì‘...")
        
        self.stats['total_articles'] = len(article_ids)
        self.stats['start_time'] = datetime.now()
        
        for i, article_id in enumerate(article_ids, 1):
            print(f"\n[{i}/{len(article_ids)}] íŒŒì‹± ì¤‘: {article_id}")
            
            verses = self.parse_single_article(article_id)
            
            if verses:
                self.results.extend(verses)
                self.stats['successful_parses'] += 1
                self.stats['total_verses'] += len(verses)
            else:
                self.stats['failed_parses'] += 1
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 50 == 0:
                success_rate = (self.stats['successful_parses'] / i) * 100
                elapsed = datetime.now() - self.stats['start_time']
                print(f"ğŸ“Š ì§„í–‰ë¥ : {i}/{len(article_ids)} ({i/len(article_ids)*100:.1f}%) | "
                      f"ì„±ê³µë¥ : {success_rate:.1f}% | ì ˆ ìˆ˜: {self.stats['total_verses']} | "
                      f"ê²½ê³¼ì‹œê°„: {elapsed}")
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            time.sleep(0.1)
        
        self.stats['end_time'] = datetime.now()
        
        print(f"\nğŸ‰ íŒŒì‹± ì™„ë£Œ!")
        print(f"  ì„±ê³µ: {self.stats['successful_parses']}/{self.stats['total_articles']}")
        print(f"  ì‹¤íŒ¨: {self.stats['failed_parses']}/{self.stats['total_articles']}")
        print(f"  ì´ ì ˆ ìˆ˜: {self.stats['total_verses']}")
        
        duration = self.stats['end_time'] - self.stats['start_time']
        print(f"  ì†Œìš” ì‹œê°„: {duration}")
    
    def save_to_excel(self, filename=None):
        """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        if not self.results:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"hochma_complete_commentaries_{timestamp}.xlsx"
        
        print(f"ğŸ’¾ ì—‘ì…€ íŒŒì¼ ì €ì¥ ì¤‘: {filename}")
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(self.results)
        
        # ì—‘ì…€ ì›Œí¬ë¶ ìƒì„±
        wb = Workbook()
        ws = wb.active
        ws.title = "í˜¸í¬ë§ˆ ì£¼ì„ ë°ì´í„°"
        
        # í—¤ë” ìŠ¤íƒ€ì¼
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        header_alignment = Alignment(horizontal="center", vertical="center")
        
        # ë°ì´í„°í”„ë ˆì„ì„ ì—‘ì…€ì— ì“°ê¸°
        for r in dataframe_to_rows(df, index=False, header=True):
            ws.append(r)
        
        # í—¤ë” ìŠ¤íƒ€ì¼ ì ìš©
        for cell in ws[1]:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = header_alignment
        
        # ì—´ ë„ˆë¹„ ì¡°ì •
        column_widths = {
            'A': 12,  # article_id
            'B': 50,  # url
            'C': 40,  # title
            'D': 15,  # commentary_name
            'E': 15,  # book_name
            'F': 8,   # chapter
            'G': 8,   # verse
            'H': 80,  # content
            'I': 12,  # content_length
            'J': 12   # pattern_type
        }
        
        for col, width in column_widths.items():
            ws.column_dimensions[col].width = width
        
        # ìš”ì•½ ì‹œíŠ¸ ì¶”ê°€
        summary_ws = wb.create_sheet(title="ìš”ì•½ ì •ë³´")
        
        summary_data = [
            ["í•­ëª©", "ê°’"],
            ["íŒŒì‹± ì¼ì‹œ", datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
            ["ì´ ê²Œì‹œê¸€ ìˆ˜", self.stats['total_articles']],
            ["ì„±ê³µí•œ ê²Œì‹œê¸€", self.stats['successful_parses']],
            ["ì‹¤íŒ¨í•œ ê²Œì‹œê¸€", self.stats['failed_parses']],
            ["ì´ ì ˆ ìˆ˜", self.stats['total_verses']],
            ["íŒŒì‹± ì‹œê°„", str(self.stats['end_time'] - self.stats['start_time']) if self.stats['end_time'] else ""],
            ["", ""],
            ["ì„±ê²½ì±…ë³„ í†µê³„", ""],
        ]
        
        # ì„±ê²½ì±…ë³„ í†µê³„
        if len(df) > 0:
            book_stats = df.groupby('book_name').agg({
                'article_id': 'nunique',
                'verse': 'count',
                'content_length': 'sum'
            }).reset_index()
            
            for _, row in book_stats.iterrows():
                summary_data.append([
                    f"  {row['book_name']}", 
                    f"ê²Œì‹œê¸€ {row['article_id']}ê°œ, ì ˆ {row['verse']}ê°œ, ì´ {row['content_length']:,}ì"
                ])
        
        for row in summary_data:
            summary_ws.append(row)
        
        # ìš”ì•½ ì‹œíŠ¸ ìŠ¤íƒ€ì¼ë§
        for cell in summary_ws[1]:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = header_alignment
        
        summary_ws.column_dimensions['A'].width = 20
        summary_ws.column_dimensions['B'].width = 50
        
        # íŒŒì¼ ì €ì¥
        wb.save(filename)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   - ì£¼ì„ ë°ì´í„°: {len(self.results)}í–‰")
        if len(df) > 0:
            print(f"   - ìš”ì•½ ì •ë³´: ì„±ê²½ì±… {len(book_stats)}ê°œ")
        
        return filename

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = CompleteBulkHochmaParser()
    
    # 1. ìµœì‹  JSON íŒŒì¼ ì‚¬ìš©
    print("1ï¸âƒ£ ê²Œì‹œê¸€ ID ìˆ˜ì§‘...")
    
    # ì™„ì „í•œ ID ëª©ë¡ íŒŒì¼ ì°¾ê¸°
    json_files = [f for f in os.listdir('.') if f.startswith('hochma_complete_ids_') and f.endswith('.json')]
    
    if json_files:
        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
        latest_file = sorted(json_files)[-1]
        print(f"   ìµœì‹  ì™„ì „ ID íŒŒì¼ ì‚¬ìš©: {latest_file}")
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            article_ids = data['article_ids']
    else:
        print("   ì™„ì „í•œ ID íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"   ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(article_ids)}ê°œ")
    
    # 2. ëª¨ë“  ê²Œì‹œê¸€ íŒŒì‹±
    print("\n2ï¸âƒ£ ëª¨ë“  ê²Œì‹œê¸€ íŒŒì‹±...")
    parser.parse_all_articles(article_ids)
    
    # 3. ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
    print("\n3ï¸âƒ£ ì—‘ì…€ íŒŒì¼ ì €ì¥...")
    filename = parser.save_to_excel()
    
    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {filename}")
    
    # ì‹¤íŒ¨í•œ IDë“¤ ì¶œë ¥
    if parser.failed_ids:
        print(f"\nâš ï¸  ì‹¤íŒ¨í•œ ê²Œì‹œê¸€ ID: {parser.failed_ids}")

if __name__ == "__main__":
    main() 