import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import re
from urllib.parse import urljoin
import json
from datetime import datetime
import pandas as pd
import os

class ExcelHochmaParser:
    def __init__(self, db_path="bible_database.db"):
        """
        ì—‘ì…€ ìš°ì„  ì €ì¥ í˜¸í¬ë§ˆ ì„±ê²½ì£¼ì„ íŒŒì„œ
        """
        self.base_url = "https://nocr.net"
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session.headers.update(self.headers)
        self.db_path = db_path
    
    def get_book_code(self, book_name):
        """ì„±ê²½ì±… ì´ë¦„ìœ¼ë¡œë¶€í„° book_code ì¡°íšŒ"""
        book_mapping = {
            'ì°½ì„¸ê¸°': 1, 'ì¶œì• êµ½ê¸°': 2, 'ë ˆìœ„ê¸°': 3, 'ë¯¼ìˆ˜ê¸°': 4, 'ì‹ ëª…ê¸°': 5,
            'ì—¬í˜¸ìˆ˜ì•„': 6, 'ì‚¬ì‚¬ê¸°': 7, 'ë£»ê¸°': 8, 'ì‚¬ë¬´ì—˜ìƒ': 9, 'ì‚¬ë¬´ì—˜í•˜': 10,
            'ì—´ì™•ê¸°ìƒ': 11, 'ì—´ì™•ê¸°í•˜': 12, 'ì—­ëŒ€ìƒ': 13, 'ì—­ëŒ€í•˜': 14, 'ì—ìŠ¤ë¼': 15,
            'ëŠí—¤ë¯¸ì•¼': 16, 'ì—ìŠ¤ë”': 17, 'ìš¥ê¸°': 18, 'ì‹œí¸': 19, 'ì ì–¸': 20,
            'ì „ë„ì„œ': 21, 'ì•„ê°€': 22, 'ì´ì‚¬ì•¼': 23, 'ì˜ˆë ˆë¯¸ì•¼': 24, 'ì˜ˆë ˆë¯¸ì•¼ì• ê°€': 25,
            'ì—ìŠ¤ê²”': 26, 'ë‹¤ë‹ˆì—˜': 27, 'í˜¸ì„¸ì•„': 28, 'ìš”ì—˜': 29, 'ì•„ëª¨ìŠ¤': 30,
            'ì˜¤ë°”ëŒœ': 31, 'ìš”ë‚˜': 32, 'ë¯¸ê°€': 33, 'ë‚˜í›”': 34, 'í•˜ë°•êµ­': 35,
            'ìŠ¤ë°”ëƒ': 36, 'í•™ê°œ': 37, 'ìŠ¤ê°€ë´': 38, 'ë§ë¼ê¸°': 39,
            'ë§ˆíƒœë³µìŒ': 40, 'ë§ˆê°€ë³µìŒ': 41, 'ëˆ„ê°€ë³µìŒ': 42, 'ìš”í•œë³µìŒ': 43, 'ì‚¬ë„í–‰ì „': 44,
            'ë¡œë§ˆì„œ': 45, 'ê³ ë¦°ë„ì „ì„œ': 46, 'ê³ ë¦°ë„í›„ì„œ': 47, 'ê°ˆë¼ë””ì•„ì„œ': 48, 'ì—ë² ì†Œì„œ': 49,
            'ë¹Œë¦½ë³´ì„œ': 50, 'ê³¨ë¡œìƒˆì„œ': 51, 'ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ': 52, 'ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ': 53,
            'ë””ëª¨ë°ì „ì„œ': 54, 'ë””ëª¨ë°í›„ì„œ': 55, 'ë””ë„ì„œ': 56, 'ë¹Œë ˆëª¬ì„œ': 57,
            'íˆë¸Œë¦¬ì„œ': 58, 'ì•¼ê³ ë³´ì„œ': 59, 'ë² ë“œë¡œì „ì„œ': 60, 'ë² ë“œë¡œí›„ì„œ': 61,
            'ìš”í•œì¼ì„œ': 62, 'ìš”í•œì´ì„œ': 63, 'ìš”í•œì‚¼ì„œ': 64, 'ìœ ë‹¤ì„œ': 65, 'ìš”í•œê³„ì‹œë¡': 66
        }
        return book_mapping.get(book_name, 999)
    
    def fetch_page(self, url, timeout=10):
        """ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response
        except requests.RequestException as e:
            print(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def extract_detailed_commentary(self, soup, url):
        """ìƒì„¸í•œ ì£¼ì„ ë°ì´í„° ì¶”ì¶œ"""
        article_data = {
            'url': url,
            'title': '',
            'commentary_name': '',
            'book_name': '',
            'chapter': '',
            'verse_commentaries': []
        }
        
        # 1. ì œëª©ì—ì„œ ì£¼ì„ëª…ê³¼ ì„±ê²½ì±… ì •ë³´ ì¶”ì¶œ
        title_found = False
        for h1 in soup.find_all('h1'):
            h1_text = h1.get_text(strip=True)
            if 'ì£¼ì„' in h1_text and ('ì¥' in h1_text or 'ê¶Œ' in h1_text or 'ì„œ' in h1_text):
                article_data['title'] = h1_text
                title_found = True
                break
        
        if not title_found:
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text(strip=True)
                if ' - ' in title_text:
                    article_data['title'] = title_text.split(' - ')[0]
                else:
                    article_data['title'] = title_text
        
        # 2. ì œëª©ì—ì„œ ì£¼ì„ëª…ê³¼ ì„±ê²½ì±… ì •ë³´ íŒŒì‹±
        if article_data['title']:
            title_match = re.search(r'([^,]+),\s*([ê°€-í£]+)\s*(\d+)ì¥', article_data['title'])
            if title_match:
                article_data['commentary_name'] = title_match.group(1).strip()
                article_data['book_name'] = title_match.group(2).strip()
                article_data['chapter'] = title_match.group(3).strip()
        
        # 3. ë³¸ë¬¸ì—ì„œ ì ˆë³„ ì£¼ì„ ì¶”ì¶œ
        content_element = soup.find(class_='xe_content') or soup.find(class_='rd_body') or soup.find(class_='rhymix_content')
        
        if content_element:
            content_text = content_element.get_text(strip=True)
            
            # 3ê°œ ì´ìƒ ë“±í˜¸ë¡œ ì ˆ êµ¬ë¶„ì ì°¾ê¸° (ìˆ˜ì •ëœ íŒ¨í„´)
            verse_pattern = r'={3,}(\d+):(\d+)'
            verse_matches = list(re.finditer(verse_pattern, content_text))
            
            if verse_matches:
                for i, match in enumerate(verse_matches):
                    chapter_num = match.group(1)
                    verse_num = match.group(2)
                    
                    start_pos = match.end()
                    if i + 1 < len(verse_matches):
                        end_pos = verse_matches[i + 1].start()
                    else:
                        end_pos = len(content_text)
                    
                    verse_commentary = content_text[start_pos:end_pos].strip()
                    verse_commentary = re.sub(r'^ì ˆ[^ê°€-í£]*', '', verse_commentary)
                    verse_commentary = verse_commentary.strip()
                    
                    if verse_commentary and len(verse_commentary) > 10:
                        article_data['verse_commentaries'].append({
                            'chapter': int(chapter_num),
                            'verse': int(verse_num),
                            'commentary': verse_commentary
                        })
            
            # ì ˆ êµ¬ë¶„ìê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì£¼ì„ìœ¼ë¡œ ì²˜ë¦¬
            if not article_data['verse_commentaries'] and len(content_text) > 100:
                if article_data['chapter']:
                    article_data['verse_commentaries'].append({
                        'chapter': int(article_data['chapter']),
                        'verse': 1,
                        'commentary': content_text
                    })
        
        return article_data
    
    def parse_to_excel(self, article_id, excel_filename=None):
        """ë‹¨ì¼ ê²Œì‹œê¸€ì„ íŒŒì‹±í•˜ì—¬ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        url = f"{self.base_url}/com_kor_hochma/{article_id}"
        
        print(f"íŒŒì‹± ì¤‘: {url}")
        
        response = self.fetch_page(url)
        if not response:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        article_data = self.extract_detailed_commentary(soup, url)
        
        if not article_data['verse_commentaries']:
            print(f"âœ— ì£¼ì„ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {url}")
            return None
        
        # Excel íŒŒì¼ëª… ìƒì„±
        if not excel_filename:
            excel_filename = f"hochma_{article_data['book_name']}_{article_data['chapter']}ì¥_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        # DataFrame ìƒì„±
        excel_data = []
        for verse_data in article_data['verse_commentaries']:
            excel_data.append({
                'ID': f"{article_data['book_name']}_{verse_data['chapter']}_{verse_data['verse']}",
                'ì£¼ì„ëª…': article_data['commentary_name'],
                'ì„±ê²½ì±…': article_data['book_name'],
                'ì„±ê²½ì±…_ì½”ë“œ': self.get_book_code(article_data['book_name']),
                'ì¥': verse_data['chapter'],
                'ì ˆ': verse_data['verse'],
                'ì£¼ì„_ë‚´ìš©': verse_data['commentary'],
                'ë²„ì „': f"{article_data['commentary_name']}-commentary",
                'ì›ë³¸_URL': article_data['url'],
                'íŒŒì‹±_ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'ë‚´ìš©_ê¸¸ì´': len(verse_data['commentary'])
            })
        
        df = pd.DataFrame(excel_data)
        
        # Excel íŒŒì¼ë¡œ ì €ì¥
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='ì£¼ì„ë°ì´í„°', index=False)
            
            # ìš”ì•½ ì‹œíŠ¸ ìƒì„±
            summary_data = {
                'í•­ëª©': ['ì£¼ì„ëª…', 'ì„±ê²½ì±…', 'ì¥', 'ì´ ì ˆ ìˆ˜', 'í‰ê·  ë‚´ìš© ê¸¸ì´', 'íŒŒì‹± ë‚ ì§œ', 'ì›ë³¸ URL'],
                'ê°’': [
                    article_data['commentary_name'],
                    article_data['book_name'],
                    article_data['chapter'],
                    len(article_data['verse_commentaries']),
                    round(df['ë‚´ìš©_ê¸¸ì´'].mean()),
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    article_data['url']
                ]
            }
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='ìš”ì•½ì •ë³´', index=False)
        
        print(f"âœ“ Excel ì €ì¥ ì™„ë£Œ: {excel_filename}")
        print(f"  - {article_data['book_name']} {article_data['chapter']}ì¥")
        print(f"  - {len(article_data['verse_commentaries'])}ê°œ ì ˆ")
        print(f"  - í‰ê·  ë‚´ìš© ê¸¸ì´: {round(df['ë‚´ìš©_ê¸¸ì´'].mean())}ì")
        
        return {
            'excel_file': excel_filename,
            'article_data': article_data,
            'dataframe': df
        }
    
    def parse_range_to_excel(self, start_id, end_id, excel_filename=None, delay=1):
        """ë²”ìœ„ ë‚´ì˜ ê²Œì‹œê¸€ë“¤ì„ íŒŒì‹±í•˜ì—¬ í•˜ë‚˜ì˜ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        print(f"ë²”ìœ„ íŒŒì‹± ì‹œì‘: {start_id} ~ {end_id}")
        
        all_excel_data = []
        parsed_count = 0
        
        for article_id in range(start_id, end_id + 1):
            url = f"{self.base_url}/com_kor_hochma/{article_id}"
            print(f"íŒŒì‹± ì¤‘: {url}")
            
            response = self.fetch_page(url)
            if not response:
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            article_data = self.extract_detailed_commentary(soup, url)
            
            if article_data['verse_commentaries']:
                for verse_data in article_data['verse_commentaries']:
                    all_excel_data.append({
                        'ID': f"{article_data['book_name']}_{verse_data['chapter']}_{verse_data['verse']}",
                        'ì£¼ì„ëª…': article_data['commentary_name'],
                        'ì„±ê²½ì±…': article_data['book_name'],
                        'ì„±ê²½ì±…_ì½”ë“œ': self.get_book_code(article_data['book_name']),
                        'ì¥': verse_data['chapter'],
                        'ì ˆ': verse_data['verse'],
                        'ì£¼ì„_ë‚´ìš©': verse_data['commentary'],
                        'ë²„ì „': f"{article_data['commentary_name']}-commentary",
                        'ì›ë³¸_URL': article_data['url'],
                        'ê²Œì‹œê¸€_ID': article_id,
                        'íŒŒì‹±_ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'ë‚´ìš©_ê¸¸ì´': len(verse_data['commentary'])
                    })
                parsed_count += 1
                print(f"  âœ“ {article_data['book_name']} {article_data['chapter']}ì¥ ({len(article_data['verse_commentaries'])}ê°œ ì ˆ)")
            else:
                print(f"  âœ— íŒŒì‹± ì‹¤íŒ¨: {article_id}")
            
            if delay > 0:
                time.sleep(delay)
        
        if not all_excel_data:
            print("íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # Excel íŒŒì¼ëª… ìƒì„±
        if not excel_filename:
            excel_filename = f"hochma_range_{start_id}_{end_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        df = pd.DataFrame(all_excel_data)
        
        # Excel íŒŒì¼ë¡œ ì €ì¥
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='ì£¼ì„ë°ì´í„°', index=False)
            
            # ì„±ê²½ì±…ë³„ ìš”ì•½
            book_summary = df.groupby('ì„±ê²½ì±…').agg({
                'ì¥': ['min', 'max', 'nunique'],
                'ì ˆ': 'count',
                'ë‚´ìš©_ê¸¸ì´': 'mean'
            }).round(1)
            book_summary.columns = ['ìµœì†Œì¥', 'ìµœëŒ€ì¥', 'ì¥ìˆ˜', 'ì´ì ˆìˆ˜', 'í‰ê· ê¸¸ì´']
            book_summary.to_excel(writer, sheet_name='ì„±ê²½ì±…ë³„_ìš”ì•½')
            
            # ì „ì²´ ìš”ì•½
            total_summary = {
                'í•­ëª©': ['ì´ ê²Œì‹œê¸€ ìˆ˜', 'ì´ ì ˆ ìˆ˜', 'ì„±ê²½ì±… ìˆ˜', 'í‰ê·  ë‚´ìš© ê¸¸ì´', 'íŒŒì‹± ë‚ ì§œ'],
                'ê°’': [
                    parsed_count,
                    len(df),
                    df['ì„±ê²½ì±…'].nunique(),
                    round(df['ë‚´ìš©_ê¸¸ì´'].mean()),
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                ]
            }
            summary_df = pd.DataFrame(total_summary)
            summary_df.to_excel(writer, sheet_name='ì „ì²´_ìš”ì•½', index=False)
        
        print(f"\nâœ“ Excel ì €ì¥ ì™„ë£Œ: {excel_filename}")
        print(f"  - ì´ {parsed_count}ê°œ ê²Œì‹œê¸€, {len(df)}ê°œ ì ˆ")
        print(f"  - í‰ê·  ë‚´ìš© ê¸¸ì´: {round(df['ë‚´ìš©_ê¸¸ì´'].mean())}ì")
        
        return {
            'excel_file': excel_filename,
            'dataframe': df,
            'parsed_count': parsed_count
        }
    
    def excel_to_database(self, excel_file):
        """ì—‘ì…€ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not os.path.exists(excel_file):
            print(f"ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_file}")
            return False
        
        # ì—‘ì…€ íŒŒì¼ ì½ê¸°
        df = pd.read_excel(excel_file, sheet_name='ì£¼ì„ë°ì´í„°')
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° í…Œì´ë¸” ìƒì„±
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # commentaries í…Œì´ë¸” ìƒì„±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS commentaries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                book_name TEXT NOT NULL,
                book_code INTEGER NOT NULL,
                chapter INTEGER NOT NULL,
                verse INTEGER NOT NULL,
                text TEXT NOT NULL,
                version TEXT NOT NULL,
                verse_title TEXT,
                commentary_name TEXT NOT NULL,
                original_url TEXT,
                parsed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ë°ì´í„° ì‚½ì…
        saved_count = 0
        for _, row in df.iterrows():
            cursor.execute('''
                INSERT OR REPLACE INTO commentaries 
                (book_name, book_code, chapter, verse, text, version, verse_title, 
                 commentary_name, original_url, parsed_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                row['ì„±ê²½ì±…'],
                row['ì„±ê²½ì±…_ì½”ë“œ'],
                row['ì¥'],
                row['ì ˆ'],
                row['ì£¼ì„_ë‚´ìš©'],
                row['ë²„ì „'],
                None,
                row['ì£¼ì„ëª…'],
                row['ì›ë³¸_URL'],
                row['íŒŒì‹±_ë‚ ì§œ']
            ))
            saved_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"âœ“ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì ˆ")
        return True


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = ExcelHochmaParser()
    
    print("ì—‘ì…€ ìš°ì„  í˜¸í¬ë§ˆ ì„±ê²½ì£¼ì„ íŒŒì„œ")
    print("=" * 50)
    
    while True:
        print("\nì„ íƒí•  ì‘ì—…:")
        print("1. ë‹¨ì¼ ê²Œì‹œê¸€ â†’ ì—‘ì…€ ì €ì¥")
        print("2. ë²”ìœ„ ê²Œì‹œê¸€ â†’ ì—‘ì…€ ì €ì¥")
        print("3. ì—‘ì…€ íŒŒì¼ â†’ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        print("4. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (1-4): ").strip()
        
        if choice == '1':
            article_id = input("ê²Œì‹œê¸€ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            excel_filename = input("Excel íŒŒì¼ëª… (ì—”í„°ì‹œ ìë™ìƒì„±): ").strip() or None
            
            if article_id:
                result = parser.parse_to_excel(article_id, excel_filename)
                if result:
                    print(f"\nğŸ“ íŒŒì¼ ìœ„ì¹˜: {result['excel_file']}")
                    
                    save_to_db = input("\në°ì´í„°ë² ì´ìŠ¤ì—ë„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    if save_to_db == 'y':
                        parser.excel_to_database(result['excel_file'])
        
        elif choice == '2':
            try:
                start_id = int(input("ì‹œì‘ ID: "))
                end_id = int(input("ì¢…ë£Œ ID: "))
                delay = float(input("ì§€ì—° ì‹œê°„(ì´ˆ, ê¸°ë³¸ 1ì´ˆ): ") or "1")
                excel_filename = input("Excel íŒŒì¼ëª… (ì—”í„°ì‹œ ìë™ìƒì„±): ").strip() or None
                
                result = parser.parse_range_to_excel(start_id, end_id, excel_filename, delay)
                if result:
                    print(f"\nğŸ“ íŒŒì¼ ìœ„ì¹˜: {result['excel_file']}")
                    
                    save_to_db = input("\në°ì´í„°ë² ì´ìŠ¤ì—ë„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    if save_to_db == 'y':
                        parser.excel_to_database(result['excel_file'])
            except ValueError:
                print("ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        elif choice == '3':
            excel_file = input("ì—‘ì…€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            parser.excel_to_database(excel_file)
        
        elif choice == '4':
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main() 