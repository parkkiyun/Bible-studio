import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime

def enhanced_find_articles():
    """ë°œê²¬ëœ ë²”ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë°€ ê²€ìƒ‰"""
    
    base_url = "https://nocr.net"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    print("ðŸ” í˜¸í¬ë§ˆ ê²Œì‹œê¸€ ì •ë°€ ê²€ìƒ‰...")
    
    all_ids = set()
    
    # ë°œê²¬ëœ ë²”ìœ„: 139800 ~ 148500
    # ì´ ë²”ìœ„ë¥¼ ë” ì„¸ë°€í•˜ê²Œ ê²€ìƒ‰
    
    search_ranges = [
        (139000, 150000, 1),  # 139000~150000ì„ 1ì”© ì¦ê°€í•˜ë©° ì „ì²´ ê²€ìƒ‰
    ]
    
    for start, end, step in search_ranges:
        print(f"\nðŸ“Š ì •ë°€ ê²€ìƒ‰: {start}~{end} (step={step})")
        
        valid_count = 0
        consecutive_fails = 0
        
        for article_id in range(start, end, step):
            url = f"{base_url}/com_kor_hochma/{article_id}"
            
            try:
                response = requests.get(url, headers=headers, timeout=5)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    title_tag = soup.find('title')
                    
                    if title_tag:
                        title_text = title_tag.get_text()
                        # í˜¸í¬ë§ˆ ì£¼ì„ì¸ì§€ í™•ì¸
                        if 'ì£¼ì„' in title_text and any(book in title_text for book in 
                            ['ì°½ì„¸ê¸°', 'ì¶œì• êµ½ê¸°', 'ë ˆìœ„ê¸°', 'ë¯¼ìˆ˜ê¸°', 'ì‹ ëª…ê¸°', 'ì—¬í˜¸ìˆ˜ì•„', 'ì‚¬ì‚¬ê¸°', 
                             'ë£»ê¸°', 'ì‚¬ë¬´ì—˜', 'ì—´ì™•ê¸°', 'ì—­ëŒ€', 'ì—ìŠ¤ë¼', 'ëŠí—¤ë¯¸ì•¼', 'ì—ìŠ¤ë”', 
                             'ìš¥ê¸°', 'ì‹œíŽ¸', 'ìž ì–¸', 'ì „ë„ì„œ', 'ì•„ê°€', 'ì´ì‚¬ì•¼', 'ì˜ˆë ˆë¯¸ì•¼', 'ì—ìŠ¤ê²”', 
                             'ë‹¤ë‹ˆì—˜', 'í˜¸ì„¸ì•„', 'ìš”ì—˜', 'ì•„ëª¨ìŠ¤', 'ì˜¤ë°”ëŒœ', 'ìš”ë‚˜', 'ë¯¸ê°€', 'ë‚˜í›”', 
                             'í•˜ë°•êµ­', 'ìŠ¤ë°”ëƒ', 'í•™ê°œ', 'ìŠ¤ê°€ëž´', 'ë§ë¼ê¸°', 'ë§ˆíƒœ', 'ë§ˆê°€', 'ëˆ„ê°€', 
                             'ìš”í•œ', 'ì‚¬ë„í–‰ì „', 'ë¡œë§ˆì„œ', 'ê³ ë¦°ë„', 'ê°ˆë¼ë””ì•„', 'ì—ë² ì†Œ', 'ë¹Œë¦½ë³´', 
                             'ê³¨ë¡œìƒˆ', 'ë°ì‚´ë¡œë‹ˆê°€', 'ë””ëª¨ë°', 'ë””ë„', 'ë¹Œë ˆëª¬', 'ížˆë¸Œë¦¬ì„œ', 'ì•¼ê³ ë³´', 
                             'ë² ë“œë¡œ', 'ìœ ë‹¤', 'ê³„ì‹œë¡']):
                            
                            all_ids.add(article_id)
                            valid_count += 1
                            consecutive_fails = 0
                            print(f"âœ“ {article_id}: {title_text[:60]}...")
                        else:
                            consecutive_fails += 1
                    else:
                        consecutive_fails += 1
                else:
                    consecutive_fails += 1
                    
            except Exception as e:
                consecutive_fails += 1
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if article_id % 1000 == 0:
                print(f"  ì§„í–‰: {article_id} (ë°œê²¬: {valid_count}ê°œ, ì—°ì†ì‹¤íŒ¨: {consecutive_fails})")
            
            # ì—°ì† 500íšŒ ì‹¤íŒ¨ì‹œ ì¢…ë£Œ (ë¹ˆ êµ¬ê°„ìœ¼ë¡œ íŒë‹¨)
            if consecutive_fails >= 500:
                print(f"  ì—°ì† ì‹¤íŒ¨ 500íšŒ, ê²€ìƒ‰ ì¤‘ë‹¨")
                break
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            time.sleep(0.05)
    
    # ê²°ê³¼ ì •ë¦¬
    final_ids = sorted(list(all_ids))
    
    print(f"\nðŸŽ‰ ì •ë°€ ê²€ìƒ‰ ê²°ê³¼:")
    print(f"  ì´ {len(final_ids)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
    if final_ids:
        print(f"  ID ë²”ìœ„: {min(final_ids)} ~ {max(final_ids)}")
    
    # JSON íŒŒì¼ë¡œ ì €ìž¥
    result = {
        'scan_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_count': len(final_ids),
        'article_ids': final_ids,
        'id_range': {
            'min': min(final_ids) if final_ids else 0,
            'max': max(final_ids) if final_ids else 0
        }
    }
    
    json_filename = f"hochma_complete_ids_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"  ì €ìž¥: {json_filename}")
    
    # ID ëª©ë¡ë„ ì¶œë ¥
    print(f"\nðŸ“‹ ë°œê²¬ëœ ê²Œì‹œê¸€ ID ëª©ë¡:")
    for i, article_id in enumerate(final_ids):
        if i % 10 == 0 and i > 0:
            print()
        print(f"{article_id:6d}", end="  ")
    print()
    
    return final_ids

if __name__ == "__main__":
    article_ids = enhanced_find_articles() 