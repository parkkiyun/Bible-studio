import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import re
from urllib.parse import urljoin
import json
from datetime import datetime
import pandas as pd
import os

class FlexibleHochmaParser:
    def __init__(self, db_path="bible_database.db"):
        """
        ìœ ì—°í•œ í˜¸í¬ë§ˆ ì„±ê²½ì£¼ì„ íŒŒì„œ - ë‹¤ì–‘í•œ ì ˆ êµ¬ë¶„ íŒ¨í„´ ì§€ì›
        """
        self.base_url = "https://nocr.net"
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session.headers.update(self.headers)
        self.db_path = db_path
        self.bible_verse_counts = self._load_bible_verse_counts()

    def _load_bible_verse_counts(self):
        try:
            with open('bible_verse_counts.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print("bible_verse_counts.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ˆ ìˆ˜ ê²€ì¦ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            return {}
    
    def get_book_code(self, book_name):
        """ì„±ê²½ì±… ì´ë¦„ìœ¼ë¡œë¶€í„° book_code ì¡°íšŒ"""
        book_mapping = {
            'ì°½ì„¸ê¸°': 1, 'ì¶œì• êµ½ê¸°': 2, 'ë ˆìœ„ê¸°': 3, 'ë¯¼ìˆ˜ê¸°': 4, 'ì‹ ëª…ê¸°': 5,
            'ì—¬í˜¸ìˆ˜ì•„': 6, 'ì‚¬ì‚¬ê¸°': 7, 'ë£»ê¸°': 8, 'ì‚¬ë¬´ì—˜ìƒ': 9, 'ì‚¬ë¬´ì—˜í•˜': 10,
            'ì—´ì™•ê¸°ìƒ': 11, 'ì—´ì™•ê¸°í•˜': 12, 'ì—­ëŒ€ìƒ': 13, 'ì—­ëŒ€í•˜': 14, 'ì—ìŠ¤ë¼': 15,
            'ëŠí—¤ë¯¸ì•¼': 16, 'ì—ìŠ¤ë”': 17, 'ìš¥ê¸°': 18, 'ì‹œí¸': 19, 'ì ì–¸': 20,
            'ì „ë„ì„œ': 21, 'ì•„ê°€': 22, 'ì´ì‚¬ì•¼': 23, 'ì˜ˆë ˆë¯¸ì•¼': 24, 'ì˜ˆë ˆë¯¸ì•¼ì• ê°€': 25,
            'ì—ìŠ¤ê²”': 26, 'ë‹¤ë‹ˆì—˜': 27, 'í˜¸ì„¸ì•„': 28, 'ìš”ì—˜': 29, 'ì•„ëª¨ìŠ¤': 30,
            'ì˜¤ë°”ëŒœ': 31, 'ìš”ë‚˜': 32, 'ë¯¸ê°€': 33, 'ë‚˜í›”': 34, 'í•˜ë°•êµ­': 35,
            'ìŠ¤ë°”ëƒ': 36, 'í•™ê°œ': 37, 'ìŠ¤ê°€ë´': 38, 'ë§ë¼ê¸°': 39,
            'ë§ˆíƒœë³µìŒ': 40, 'ë§ˆê°€ë³µìŒ': 41, 'ëˆ„ê°€ë³µìŒ': 42, 'ìš”í•œë³µìŒ': 43, 'ì‚¬ë„í–‰ì „': 44,
            'ë¡œë§ˆì„œ': 45, 'ê³ ë¦°ë„ì „ì„œ': 46, 'ê³ ë¦°ë„í›„ì„œ': 47, 'ê°ˆë¼ë””ì•„ì„œ': 48, 'ì—ë² ì†Œì„œ': 49,
            'ë¹Œë¦½ë³´ì„œ': 50, 'ê³¨ë¡œìƒˆì„œ': 51, 'ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ': 52, 'ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ': 53,
            'ë””ëª¨ë°ì „ì„œ': 54, 'ë””ëª¨ë°í›„ì„œ': 55, 'ë””ë„ì„œ': 56, 'ë¹Œë ˆëª¬ì„œ': 57,
            'íˆë¸Œë¦¬ì„œ': 58, 'ì•¼ê³ ë³´ì„œ': 59, 'ë² ë“œë¡œì „ì„œ': 60, 'ë² ë“œë¡œí›„ì„œ': 61,
            'ìš”í•œì¼ì„œ': 62, 'ìš”í•œì´ì„œ': 63, 'ìš”í•œì‚¼ì„œ': 64, 'ìœ ë‹¤ì„œ': 65, 'ìš”í•œê³„ì‹œë¡': 66
        }
        return book_mapping.get(book_name, 999)
    
    def fetch_page(self, url, timeout=10):
        """ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response
        except requests.RequestException as e:
            print(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def detect_verse_pattern(self, content_text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì ˆ êµ¬ë¶„ íŒ¨í„´ ê°ì§€"""
        patterns = {
            'equals_4': r'====(\d+):(\d+)ì ˆ?',
            'equals_3': r'===(\d+):(\d+)ì ˆ?',
            'equals_any': r'={3,}(\d+):(\d+)ì ˆ?',
            'line_start': r'^(\d+):(\d+)ì ˆ?$',
        }
        
        pattern_counts = {}
        for pattern_name, pattern in patterns.items():
            if pattern_name == 'line_start':
                lines = content_text.split('\n')
                matches = []
                for i, line in enumerate(lines):
                    line = line.strip()
                    match = re.match(pattern, line)
                    if match:
                        matches.append((i, match.group(1), match.group(2)))
                pattern_counts[pattern_name] = matches
            else:
                matches = list(re.finditer(pattern, content_text))
                pattern_counts[pattern_name] = [(m.start(), m.group(1), m.group(2)) for m in matches]
        
        best_pattern = None
        max_count = 0
        
        for pattern_name, matches in pattern_counts.items():
            if len(matches) > max_count:
                max_count = len(matches)
                best_pattern = pattern_name
        
        return best_pattern, pattern_counts[best_pattern] if best_pattern else []
    
    def parse_verses_by_pattern(self, content_text, pattern_type, matches, book_name, chapter_num):
        """íŒ¨í„´ì— ë”°ë¼ ì ˆë³„ë¡œ ë¶„í• í•˜ê³ , ì˜ˆìƒ ì ˆ ìˆ˜ì— ë§ì¶° ì •ê·œí™”"""
        parsed_verses = {}
        
        if pattern_type == 'line_start':
            lines = content_text.split('\n')
            for i, (line_idx, chapter, verse) in enumerate(matches):
                current_verse = int(verse)
                current_chapter = int(chapter)
                
                # ë‹¤ìŒ ì ˆì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
                next_line_idx = len(lines)
                if i + 1 < len(matches):
                    next_line_idx = matches[i + 1][0]
                
                verse_content_lines = lines[line_idx + 1:next_line_idx]
                verse_content = '\n'.join(verse_content_lines).strip()
                verse_content = re.sub(r'\n\s*\n', '\n\n', verse_content)
                
                if verse_content.strip():
                    parsed_verses[(current_chapter, current_verse)] = verse_content
        else:
            for i, (pos, chapter, verse) in enumerate(matches):
                current_verse = int(verse)
                current_chapter = int(chapter)
                
                start_pos = content_text.find('\n', pos)
                if start_pos == -1:
                    continue
                start_pos += 1
                
                if i + 1 < len(matches):
                    end_pos = matches[i + 1][0]
                else:
                    end_pos = len(content_text)
                
                verse_content = content_text[start_pos:end_pos].strip()
                verse_content = re.sub(r'\n\s*\n', '\n\n', verse_content)
                
                if verse_content.strip():
                    parsed_verses[(current_chapter, current_verse)] = verse_content

        # ì˜ˆìƒ ì ˆ ìˆ˜ì— ë§ì¶° ì •ê·œí™”
        normalized_verse_commentaries = []
        if book_name in self.bible_verse_counts and chapter_num <= len(self.bible_verse_counts[book_name]):
            expected_verses_in_chapter = self.bible_verse_counts[book_name][chapter_num - 1]
            for i in range(1, expected_verses_in_chapter + 1):
                commentary_content = parsed_verses.get((chapter_num, i), '[ëˆ„ë½ëœ ì ˆ]')
                normalized_verse_commentaries.append({
                    'chapter': chapter_num,
                    'verse': i,
                    'commentary': commentary_content
                })
        else:
            # ì˜ˆìƒ ì ˆ ìˆ˜ ì •ë³´ê°€ ì—†ìœ¼ë©´ íŒŒì‹±ëœ ì ˆë§Œ ì‚¬ìš©
            for (chap, ver), content in parsed_verses.items():
                normalized_verse_commentaries.append({
                    'chapter': chap,
                    'verse': ver,
                    'commentary': content
                })
            # íŒŒì‹±ëœ ì ˆì´ ì—†ëŠ”ë° ì „ì²´ ì¥ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²½ìš°
            if not normalized_verse_commentaries and len(content_text) > 100:
                normalized_verse_commentaries.append({
                    'chapter': chapter_num,
                    'verse': 1,
                    'commentary': content_text
                })

        return normalized_verse_commentaries
    
    def extract_flexible_commentary(self, soup, url):
        """ìœ ì—°í•œ ì£¼ì„ ë°ì´í„° ì¶”ì¶œ"""
        article_data = {
            'url': url,
            'title': '',
            'commentary_name': '',
            'book_name': '',
            'chapter': '',
            'verse_commentaries': [],
            'pattern_info': {}
        }
        
        title_found = False
        for h1 in soup.find_all('h1'):
            h1_text = h1.get_text(strip=True)
            if 'ì£¼ì„' in h1_text and ('ì¥' in h1_text or 'ê¶Œ' in h1_text or 'ì„œ' in h1_text):
                article_data['title'] = h1_text
                title_found = True
                break
        
        if not title_found:
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text(strip=True)
                if ' - ' in title_text:
                    article_data['title'] = title_text.split(' - ')[0]
                else:
                    article_data['title'] = title_text
        
        if article_data['title']:
            title_match = re.search(r'([^,]+),\s*([ê°€-í£]+)\s*(\d+)ì¥', article_data['title'])
            if title_match:
                article_data['commentary_name'] = title_match.group(1).strip()
                article_data['book_name'] = title_match.group(2).strip()
                article_data['chapter'] = title_match.group(3).strip()
        
        content_element = soup.find(class_='xe_content') or soup.find(class_='rd_body') or soup.find(class_='rhymix_content')
        
        if content_element:
            content_text = content_element.get_text('\n', strip=True)
            pattern_type, matches = self.detect_verse_pattern(content_text)
            
            if pattern_type and matches:
                print(f"ê°ì§€ëœ íŒ¨í„´: {pattern_type} ({len(matches)}ê°œ ì ˆ)")
                article_data['pattern_info'] = {
                    'type': pattern_type,
                    'count': len(matches)
                }
                verses = self.parse_verses_by_pattern(content_text, pattern_type, matches, article_data['book_name'], int(article_data['chapter']))
                article_data['verse_commentaries'] = verses
            
            elif len(content_text) > 100:
                if article_data['chapter']:
                    article_data['verse_commentaries'].append({
                        'chapter': int(article_data['chapter']),
                        'verse': 1,
                        'commentary': content_text
                    })
                    article_data['pattern_info'] = {
                        'type': 'whole_chapter',
                        'count': 1
                    }
        
        return article_data
    
    def parse_to_excel(self, article_id, excel_filename=None):
        """ë‹¨ì¼ ê²Œì‹œê¸€ì„ íŒŒì‹±í•˜ì—¬ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        url = f"{self.base_url}/com_kor_hochma/{article_id}"
        print(f"íŒŒì‹± ì¤‘: {url}")
        
        response = self.fetch_page(url)
        if not response:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        article_data = self.extract_flexible_commentary(soup, url)
        
        if not article_data['verse_commentaries']:
            print(f"ì£¼ì„ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {url}")
            return {
                'excel_file': None,
                'article_data': article_data,
                'dataframe': pd.DataFrame()
            }
        
        # ì •ê·œí™”ëœ ì ˆ ëª©ë¡ ìƒì„±
        normalized_verse_commentaries = []
        book_name = article_data.get('book_name')
        chapter = article_data.get('chapter')

        if book_name and chapter and self.bible_verse_counts:
            chapter_int = int(chapter)
            if book_name in self.bible_verse_counts and chapter_int <= len(self.bible_verse_counts[book_name]):
                expected_verses_in_chapter = self.bible_verse_counts[book_name][chapter_int - 1]
                
                parsed_verses_map = {v['verse']: v for v in article_data['verse_commentaries']}
                
                for i in range(1, expected_verses_in_chapter + 1):
                    if i in parsed_verses_map:
                        normalized_verse_commentaries.append(parsed_verses_map[i])
                    else:
                        # ëˆ„ë½ëœ ì ˆì— ëŒ€í•œ í”Œë ˆì´ìŠ¤í™€ë” ì¶”ê°€
                        normalized_verse_commentaries.append({
                            'chapter': chapter_int,
                            'verse': i,
                            'commentary': '[ëˆ„ë½ëœ ì ˆ]'
                        })
            else:
                # ì„±ê²½ì±… ë˜ëŠ” ì¥ ì •ë³´ê°€ bible_verse_countsì— ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©
                normalized_verse_commentaries = article_data['verse_commentaries']
        else:
            # bible_verse_countsê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©
            normalized_verse_commentaries = article_data['verse_commentaries']

        article_data['verse_commentaries'] = normalized_verse_commentaries

        # Excel íŒŒì¼ëª… ìƒì„±
        if not excel_filename:
            pattern_info = article_data['pattern_info'].get('type', 'unknown')
            excel_filename = f"hochma_{article_data['book_name']}_{article_data['chapter']}ì¥_{pattern_info}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        # DataFrame ìƒì„±
        excel_data = []
        for verse_data in article_data['verse_commentaries']:
            excel_data.append({
                'ID': f"{article_data['book_name']}_{verse_data['chapter']}_{verse_data['verse']}",
                'ì£¼ì„ëª…': article_data['commentary_name'],
                'ì„±ê²½ì±…': article_data['book_name'],
                'ì„±ê²½ì±…_ì½”ë“œ': self.get_book_code(article_data['book_name']),
                'ì¥': verse_data['chapter'],
                'ì ˆ': verse_data['verse'],
                'ì£¼ì„_ë‚´ìš©': verse_data['commentary'],
                'ë²„ì „': f"{article_data['commentary_name']}-commentary",
                'ì›ë³¸_URL': article_data['url'],
                'íŒŒì‹±_ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'ë‚´ìš©_ê¸¸ì´': len(verse_data['commentary']),
                'íŒ¨í„´_ìœ í˜•': article_data['pattern_info'].get('type', 'unknown')
            })
        
        df = pd.DataFrame(excel_data)
        
        # Excel íŒŒì¼ë¡œ ì €ì¥
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='ì£¼ì„ë°ì´í„°', index=False)
            
            # ìš”ì•½ ì‹œíŠ¸ ìƒì„±
            summary_data = {
                'í•­ëª©': ['ì£¼ì„ëª…', 'ì„±ê²½ì±…', 'ì¥', 'ì´ ì ˆ ìˆ˜', 'í‰ê·  ë‚´ìš© ê¸¸ì´', 'íŒ¨í„´ ìœ í˜•', 'íŒŒì‹± ë‚ ì§œ', 'ì›ë³¸ URL'],
                'ê°’': [
                    article_data['commentary_name'],
                    article_data['book_name'],
                    article_data['chapter'],
                    len(article_data['verse_commentaries']),
                    round(df['ë‚´ìš©_ê¸¸ì´'].mean()) if not df.empty else 0,
                    article_data['pattern_info'].get('type', 'unknown'),
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    article_data['url']
                ]
            }
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='ìš”ì•½ì •ë³´', index=False)
        
        print(f"Excel ì €ì¥ ì™„ë£Œ: {excel_filename}")
        print(f"  - {article_data['book_name']} {article_data['chapter']}ì¥")
        print(f"  - {len(article_data['verse_commentaries'])}ê°œ ì ˆ ({article_data['pattern_info'].get('type', 'unknown')} íŒ¨í„´)")
        print(f"  - í‰ê·  ë‚´ìš© ê¸¸ì´: {round(df['ë‚´ìš©_ê¸¸ì´'].mean()) if not df.empty else 0}ì")
        
        return {
            'excel_file': excel_filename,
            'article_data': article_data,
            'dataframe': df
        }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = FlexibleHochmaParser()
    
    print("ìœ ì—°í•œ í˜¸í¬ë§ˆ ì„±ê²½ì£¼ì„ íŒŒì„œ (ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›)")
    print("=" * 60)
    
    while True:
        print("\nì„ íƒí•  ì‘ì—…:")
        print("1. ë‹¨ì¼ ê²Œì‹œê¸€ íŒŒì‹± (ìë™ íŒ¨í„´ ê°ì§€)")
        print("2. ì—‘ì…€ íŒŒì¼ â†’ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        print("3. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == '1':
            article_id = input("ê²Œì‹œê¸€ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            excel_filename = input("Excel íŒŒì¼ëª… (ì—”í„°ì‹œ ìë™ìƒì„±): ").strip() or None
            
            if article_id:
                result = parser.parse_to_excel(article_id, excel_filename)
                if result and result['excel_file']:
                    print(f"\nğŸ“ íŒŒì¼ ìœ„ì¹˜: {result['excel_file']}")
                    
                    save_to_db = input("\në°ì´í„°ë² ì´ìŠ¤ì—ë„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    if save_to_db == 'y':
                        parser.excel_to_database(result['excel_file'])
        
        elif choice == '2':
            excel_file = input("ì—‘ì…€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            parser.excel_to_database(excel_file)
        
        elif choice == '3':
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()